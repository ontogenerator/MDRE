---
title: "Multi-dimensional reward evaluation in mice"
header-includes:
   - \usepackage{lineno}
   - \linenumbers
   # - \usepackage{float} \floatplacement{figure}{H}
   - \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{A\arabic{figure}}}
always_allow_html: yes
output:   
  bookdown::pdf_document2:
  # word_document
    number_sections: no
    toc: no
bibliography: MDRE.bib
---
Vladislav Nachev^1,\*,&#8224;,\S^, Marion Rivalan^1,&#8224;,\S^, York Winter^1,\S^

^1^ Humboldt University, Berlin, Germany

**^\*^For correspondence:** vladislav.nachev\@charite.de  
^&#8224;^These authors contributed equally to this work

**Present Address:** ^\S^Dept. of Biology, Humboldt University, Philippstr. 13, 10099 Berlin, Germany

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Abstract

The experiments in this study were designed to test how mice use information from two reward dimensions (probability and volume) when deciding where to forage.  
[Please provide an abstract of no more than 150 words. Your abstract should explain the main contributions of your article, and should not contain any material that is not included in the main text. ]

# Introduction

[@levy_root_2012]
[@kacelnik_risky_1998]
[@rosenstrom_scalar_2016]
[@rivalan_principles_2017]
[@matell_timing_2014]
The partial preference observed in choice experiments can be explained by profitability matching [@kacelnik_central_1984], which states that animals proportionally allocate their effort depending on the relative pay-off of the options. 

# Results

```{r, include = FALSE}
library(tidyverse)
library(gridExtra)
# library(grid)
library(lubridate)
library(lme4)
library(coin)
library(kableExtra)
library(Hmisc)
library(broom)
# library(broom.mixed)

miceChoices <- read.csv2(file = "data/ChoicesOutput.csv", header = TRUE, dec = ".", sep = ";",
                            na.strings = "NA") %>%
  mutate(vol = if_else(cohort == 2, vol * 4.8, vol))

miceChoices <- miceChoices %>%
  group_by(IdLabel, day) %>%
  filter(hour(DateTime) < 9 | hour(DateTime) > 15) %>% # due to older program version the initial visits
  # before the starting could not be flagged and therefore could not be removed by CSVreader.R
  # therefore they need to be removed here
  group_by(IdLabel, day) %>%
  mutate(count = 1:n(),
         revcount = n():1,
         DateTime = as.POSIXct(DateTime),
         cohort = factor(cohort),
         relcount = cumsum(rel)) %>%
  ungroup()

summaries <- miceChoices %>%
  group_by(cond, IdLabel, experiment, cohort, rint) %>%
  # take all visits after the first 150 visits at the relevant dispensers:
  filter(relcount > 150) %>%
  # in the following lines is an alternative cut-off point, which leads to very similar results
  # take the last 100 visits at any of the dispensers:
  # filter(revcount < 101) %>%
  summarise(performance = mean(prof, na.rm = TRUE), sampling = 1 - mean(rel),
            Ntot = n(), Nrel = sum(rel), successes_perf = sum(prof, na.rm = T),
            successes_sampl =  Ntot - Nrel) %>%
  ungroup() %>%
  mutate(cohort = factor(cohort))

total_vol <- miceChoices %>%
  group_by(experiment, day, cond, cohort, IdLabel) %>%
  summarise(vol_consumed = sum(vol),
            n_vis = n(),
            drank_little = ifelse(vol_consumed < 1000, 1, 0)) %>%
  mutate(cage = ifelse(cohort == 2, 2, 1))

drank_little <- total_vol %>%
  filter(!str_detect(cond, "r")) %>%
  group_by(IdLabel, cage) %>%
  summarise(perc_insuf = mean(drank_little),
            tot_insuf = sum(drank_little))

exp3_tab <- miceChoices %>%
  filter(experiment == 3, !str_detect(cond,"[r]")) %>%
  group_by(cohort, cond) %>%
  summarise(vol = max(volm), prob = max(prob)) %>%
  group_by(cohort) %>%
  mutate(vol_prop = vol/max(vol),
         maxvol = max(vol),
         prob_prop = prob/max(prob),
         maxprob = max(prob),
         dim = str_sub(cond, 1, 1),
         bg_level = ifelse(dim == "P", vol_prop, prob_prop)) %>%
  ungroup() %>%
  mutate(cohort = factor(cohort)) %>%
  droplevels()

bg_vols <- exp3_tab %>%
  filter(dim == "P", cohort == 1) %>%
  select(bg_level, vol) %>%
  round(3)

bg_lev_vol <- paste(bg_vols$bg_level, collapse = ", ")
bg_vol <- paste(bg_vols$vol, collapse = ", ")

exp1 <- summaries %>% filter(!str_detect(cond, "[r]"), experiment == 1)
BVP1 <- exp1 %>%
  filter(cond == "BVP1")
exp2 <- summaries %>% filter(!str_detect(cond, "[r]"), experiment == 2) %>%
  bind_rows(BVP1)
exp3 <- summaries %>% filter(!str_detect(cond, "[r]"), experiment == 3) %>%
  inner_join(exp3_tab) %>%
  droplevels()
exp4 <- summaries %>% filter(!str_detect(cond, "[r]"), experiment == 4)

sesoi <- 0.1 # smallest effect size of interest
```

In order to test how (contradicting) information from two different dimensions is integrated and weighed, we performed a series of choice experiments (1-4, in chronological order) with mice in automated group cages [@rivalan_principles_2017]. The cages were outfitted with four computer-controlled liquid dispensers that delivered drinking water as a reward. During each of the 18h-long drinking sessions each mouse had access to all dispensers, but received rewards at only two of them. The two rewarding dispensers differed on one or both reward dimensions, probability and volume [@rivalan_principles_2017]. An overview of the differences between choice options in the different experimental conditions is given in Table \@ref(tab:conds). All experiments were conducted with three different cohorts of eight mice each. Cohort 2 was housed in a different automated group cage than cohorts 1 and 3 (See Animals, Materials, and Methods for differences between cages).

## Experiment 1  
In the baseline conditions rewards only differed on one dimension (the relevant dimension), but not on the other dimension (the background dimension). For example, in the BVP1 condition (read as baseline for volume at probability 1), both options had the same probability of 0.2, but one option had a volume of 4 $\mu$L and the other, a volume of 20 $\mu$L (Table \@ref(tab:conds)). Based on previous experiments [@rivalan_principles_2017], we expected a baseline difference between 4 $\mu$L and 20 $\mu$L volumes to result in a similar discrimination performance (relative preference for the superior option) compared to a baseline difference between probabilities 0.2 and 0.5. In the C (congruent) condition one option was superior to the other on both dimensions. Finally, in the I (incongruent) condition each of the two options was superior to the other on one of the two reward dimensions. Since the differences on both dimensions were chosen to be comparable, we expected the mean discrimination performance in the incongruent condition to be at chance level (0.5).  
In experiment 1 and in all subsequent experiments, each mouse had its own individual sequence of conditions, but each condition was followed by a reversal in the next drinking session, with a spatial inversion of the two rewarding dispensers. In order to investigate how the two reward dimensions contributed towards choice, we looked at the contrasts between the baselines (when only one dimension was relevant) to the conditions when the two dimensions were congruent or incongruent to each other. We used equivalence tests [@lakens_equivalence_2017] with an *a priori* smallest effect size of interest (sesoi) of `r sesoi`, i.e. we only considered absolute differences of at least `r sesoi` percentage points to be of biological relevance. Smaller differences, regardless of their statistical significance using other tests, were considered to be trivial.

```{r conds}
conds_tab <- miceChoices %>%
  ungroup() %>%
  filter(!str_detect(cond,"[r]"), cohort != 2, between(vol, 1, 20),
         experiment < 4, prob > 0) %>%
  select(experiment, cond, vol, prob) %>%
  distinct() %>%
  arrange(experiment, cond, vol, prob)

conds_tab <- conds_tab %>%
  mutate(odd = row_number(cond) %% 2, ret = vol*prob, vol2 = lead(vol), prob2 = lead(prob), ret2 = vol2*prob2, relret = ret/ret2) %>%
  filter(odd == 1) %>%
  select(-odd)

conds_tab %>%
  mutate(cond =
           if_else(cond == "BVP1", paste0(cond, kableExtra::footnote_marker_alphabet(4)),
                   as.character(cond))) %>%
  knitr::kable(booktabs = TRUE,
               caption = "Overview of the experimental conditions in all four experiments.",
               col.names = c(paste0("experiment", kableExtra::footnote_marker_alphabet(1)),
                             "condition",
                             paste0("volume", kableExtra::footnote_marker_alphabet(2)),
                             "probability",
                             paste0("return", kableExtra::footnote_marker_alphabet(3)),
                             paste0("volume", kableExtra::footnote_marker_alphabet(2)),
                             "probability",
                             paste0("return", kableExtra::footnote_marker_alphabet(3)),
                             "relative return"),
               escape = F) %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F) %>%
  kableExtra::add_header_above(c(" " = 2, "option A" = 3, "option B" = 3, "A/B" = 1)) %>%
  kableExtra::footnote(alphabet =
                         c("conditions in experiment 1 and 4 were identical; only conditions for experiment 1 are shown here for brevity; ",
                           "the volumes (in microliters) shown are for cohorts 1 and 3. In cohort 2 the volumes were 4.7 instead of 4, 9.4 instead of 10, 14.0 instead of 15, and 20.3 instead of 20 microliters; ",
                           "return rate (expected value); ",
                           "condition BVP1 in experiment 1 was not repeated in experiment 2, but instead the results from experiment 1 were reused in further analyses"),
                       threeparttable = T)
```
  
(ref:labresoverview) **Overview of discrimination performance for all mice in all experiments.** Experiments 1 through 4 are shown in different panels (1-4). Each symbol is the mean discrimination performance of an individual mouse over two presentations of the same condition (original and reversal). The experimental conditions are described in detail in Table \@ref(tab:conds). The discrimination performance gives the relative visitation rate of the more profitable option, or, in the incongruent condition, the option with the higher volume. Dashed line gives the chance level of 0.5. Data are shown in different colors for three different cohorts of eight mice each (total *N* = 24). Data from the same individuals are connected with lines. Cohort 2 (green symbols and lines) was tested in a different cage set-up than cohorts 1 and 2 (see Animals, Materials, and Methods for details). 

```{r resoverview, fig.cap="(ref:labresoverview)"}
ggexp12_4 <- summaries %>%
  filter(!str_detect(cond, "[r]"), experiment != 3) %>%
  ggplot() +
  geom_point(aes(cond, performance, color = cohort, group = IdLabel)) +
  geom_line(aes(cond, performance, color = cohort, group = IdLabel)) +
  facet_grid(experiment ~ .) + xlab("condition") + ylab("discrimination performance") +
  theme_bw() + scale_color_viridis_d() + guides(color = FALSE) +
  theme(strip.text.y = element_text(angle = 0)) +
  geom_hline(yintercept = 0.5, linetype = 2)
  

ggexp3 <- exp3 %>%
  ggplot() +
  geom_point(aes(cond, performance, color = cohort, group = IdLabel)) +
  geom_line(aes(cond, performance, color = cohort, group = IdLabel)) +
  facet_grid(experiment ~ .) + xlab("condition") +
  scale_y_continuous(name = "", breaks = seq(0, 1, 0.25)) +
  theme_bw() + scale_color_viridis_d() + theme(legend.position = "bottom") +
  theme(strip.text.y = element_text(angle = 0)) +
  geom_hline(yintercept = 0.5, linetype = 2)
  

get_legend <- function(a.gplot){ 
  tmp <- ggplot_gtable(ggplot_build(a.gplot)) 
  leg <- which(map(tmp$grobs, "name") == "guide-box") 
  legend <- tmp$grobs[[leg]] 
  return(legend)} 

leg <- get_legend(ggexp3)

lay <- rbind(c(1, 3),
             c(1, 2),
             c(1, NA))

grid.arrange(ggexp12_4, ggexp3 + guides(color = FALSE), leg, layout_matrix = lay,
             heights = c(1, 1.3, 1))
```

(ref:labresexp1) **Difference between discrimination performance in the baseline conditions and in the congruent and incongruent conditions in experiment 1**. Symbols show the individual differences in discrimination performance for the given conditions of each individual mouse (*N* = 24). Mice from different cohorts are shown in different colors. Large blue symbols give the means and the blue vertical lines the 97%-confidence intervals from non-parametric bootstraps (correction of 90% CI for multiple comparisons). When the confidence intervals lie completely within the smallest effect size of interest (sesoi) interval bounded by the dashed lines, there is statistical support for equivalence [@lakens_equivalence_2017]. When the confidence intervals do not cross the zero line, there is statistical support for difference. The discrimination performances in the baseline conditions were calculated from the mean values from the two different baseline conditions for each reward dimension (volume and probability), i.e. BP was the mean of BPV1 and BPV2, and BV was the mean of BVP1 and BVP2 (Table \@ref(tab:conds)). The discrimination performance in the incongruent condition was calculated as the relative preference for the higher probability dispenser when contrasted with the probability baseline (I - BP) and for the higher volume dispenser when contrasted with the volume baseline (I - BV).     

```{r exp1, fig.cap="(ref:labresexp1)"}
exp1_simpl <- exp1 %>%
  ungroup() %>%
  mutate(cond = as.factor(ifelse(str_length(cond) == 4, str_sub(cond, 1, 2),
                                 as.character(cond)))) %>%
  group_by(cond, IdLabel, cohort) %>%
  summarise(performance = mean(performance))

exp1_diff <- exp1_simpl %>%
  spread(cond, performance) %>%
  mutate("BP - BV" = BP - BV, "C - BP" = C - BP, "C - BV" = C - BV,
         "I - BP" = 1 - I - BP, "I - BV" = I - BV) %>%
  select(-BV, -BP, -C, -I) %>%
  gather(cond, difference, -cohort, -IdLabel) %>%
  mutate(rel_dim = str_sub(cond, -1))

n_compare <- exp1_diff %>%
  ungroup() %>%
  filter(cond != "BP - BV") %>%
  pull(cond) %>%
  unique() %>%
  length()

set.seed(42)
mean_inc1 <- exp1 %>%
  filter(cond == "I") %>%
  pull(performance) %>%
  mean_cl_boot() %>%
  mutate_all(round, 3)


mean_ibp <- exp1_diff %>%
  filter(cond == "I - BP") %>%
  pull(difference) %>%
  mean_cl_boot() %>%
  mutate_all(round, 3)

mean_ibv <- exp1_diff %>%
  filter(cond == "I - BV") %>%
  pull(difference) %>%
  mean_cl_boot() %>%
  mutate_all(round, 3)

alpha_1 <- 0.05
n_compare = 2
alpha_2 <- 0.05 / (n_compare - 1) # alpha corrected for multiple comparisons

exp1_diff %>%
  filter(cond != "BP - BV") %>%
  ggplot(aes(cond, difference)) +
  geom_hline(yintercept = sesoi, linetype = 2) +
  geom_hline(yintercept = -sesoi, linetype = 2) +
  stat_summary(aes(cond, difference), fun.data = mean_cl_boot,
               fun.args = list(conf.int = 1 - 2*(alpha_2)),
               color = "darkblue", size = 1) +
  theme_bw() + geom_jitter(aes(color = cohort), alpha = 0.7, width = 0.1) +
  xlab("") + scale_color_viridis_d()

```

  
Compared to the baselines, mice showed an increase in discrimination performance in the congruent condition and a decrease in performance in the incongruent condition (Fig. \@ref(fig:exp1)). Contrary to our expectations, the trade-off between volume and probability did not abolish preference in the incongruent condition (Fig. \@ref(fig:resoverview)), with a discrimination performance significantly higher than the chance level of 0.5 (lower 95%CI < mean < upper 95%CI, `r paste(mean_inc1$ymin, mean_inc1$y, mean_inc1$ymax, sep = " < ")`). Thus, the volume dimension exerted a stronger influence on choice, at least in absolute terms.  
 
## Experiment 2
In previous experiments [@rivalan_principles_2017], we had shown that the relative stimulus intensity, i.e. the absolute difference between two options divided by their mean, was a good predictor of discrimination performance for both volume and probability differences. Another finding from these experiments was that, at least initially, mice responded less strongly to  differences in volume than to differences in probability, despite equivalence in return rates [@rivalan_principles_2017]. We tried to correct for this in experiment 1 by selecting options with a higher relative intensity for volume (4 $\mu$L vs. 20 $\mu$L, rel.int. = 1.33) than for probability (0.2 vs. 0.5, rel.int. = 0.857). In order to test whether we had over-corrected for decreased sensitivity to volume in experiment 1, we performed a slightly modified version, experiment 2, which was the same, but with probability of 1 instead of 0.5 in every choice option from experiment 1 (Table \@ref(tab:conds)). Thus, with the two choice options having the same relative intensities (rel.int. = 1.33) and being equivalent in return rates, we expected the discrimination performance in the incongruent condition to be at chance level if both dimensions were equally weighed and equally perceived. On the other hand, if mice were less sensitive for volume than for probability differences as in our previous experiments, then the discrimination performance in the incongruent condition should be skewed towards probability (< 0.5).  

(ref:labresexp2) **Difference between discrimination performance in the baseline conditions and in the congruent and incongruent conditions in experiment 2**. Same notation as in Fig. \@ref(fig:exp1). The discrimination performances in the baseline conditions were calculated from the mean values from the two different baseline conditions for each reward dimension (volume and probability), i.e. BP was the mean of BPV1 and BPV2, and BV was the mean of BVP1 and BVP2, where the values for condition BVP1 were taken from experiment 1 (Table \@ref(tab:conds)). The discrimination performance in the incongruent condition was calculated as the relative preference for the higher probability dispenser when contrasted with the probability baseline (I - BP) and for the higher volume dispenser when contrasted with the volume baseline (I - BV).

```{r exp2, fig.cap="(ref:labresexp2)"}
exp2_simpl <- exp2 %>%
  ungroup() %>%
  mutate(cond = as.factor(ifelse(str_length(cond) == 4, str_sub(cond, 1, 2),
                                 as.character(cond)))) %>%
  group_by(cond, IdLabel, cohort) %>%
  summarise(performance = mean(performance))

exp2_diff <- exp2_simpl %>%
  spread(cond, performance) %>%
  mutate("BP - BV" = BP - BV, "C - BP" = C - BP, "C - BV" = C - BV,
         "I - BP" = 1 - I - BP, "I - BV" = I - BV) %>%
  select(-BV, -BP, -C, -I) %>%
  gather(cond, difference, -cohort, -IdLabel)

set.seed(42)
mean_inc2 <- exp2 %>%
  filter(cond == "I") %>%
  pull(performance) %>%
  mean_cl_boot() %>%
  mutate_all(round, 3)

exp2_diff %>%
  filter(cond != "BP - BV") %>%
  ggplot(aes(cond, difference)) +
  geom_hline(yintercept = sesoi, linetype = 2) + geom_hline(yintercept = -sesoi, linetype = 2) +
  stat_summary(aes(cond, difference), fun.data = mean_cl_boot,
               fun.args = list(conf.int = 1 - 2*(alpha_2)),
               color = "darkblue", size = 1) +
  theme_bw() + geom_jitter(aes(color = cohort), alpha = 0.7, width = 0.1) +
  xlab("") + scale_color_viridis_d()

```
  
In contrast to experiment 1, in experiment 2 mice showed an increase in discrimination performance in the congruent condition only when compared to the volume baseline, but not when compared to the probability baseline (Fig. \@ref(fig:exp2)). As in experiment 1, the discrimination performance in the incongruent condition was lower than in either of the two baselines (Fig. \@ref(fig:exp2)). Although the discrimination performance in the incongruent condition was again different from 0.5 (`r paste(mean_inc2$ymin, mean_inc2$y, mean_inc2$ymax, sep = " < ")`), it was lower than chance, thus skewed towards probability (Fig. \@ref(fig:resoverview)).


## Experiment 3
In the previous experiments we used two different baseline conditions for each dimension (BPV1, BPV2, BVP1, and BVP2, Table \@ref(tab:conds)), in order to exhaust all combinations of reward stimuli and balance the experimental design. But could it be that the level of the background dimension despite being the same across choice options nevertheless affected the discrimination performance on the relevant dimension? Researches have proposed that in multi-dimensional choice the decision process can be considerably simplified if differences that are (nearly) equal are not evaluated but ignored [@tversky_intransitivity_1969; @shafir_intransitivity_1994; @shafir_comparative_2014]. Thus we can predict that regardless of the level of the background dimension, the discrimination performance on the relevant dimension should remain constant. Alternatively, animals could use all information from every reward dimensions for the estimation of a single value (utility), in what some authors refer to as "absolute reward evaluation" [@tversky_intransitivity_1969; @shafir_intransitivity_1994; @shafir_comparative_2014]. Since the utility curve is generally assumed to progressively increase with the increase in any given good, but with a decreasing slope [@kahneman_prospect_1979; @kenrick_deep_2009; but see also @kacelnik_risky_1998], we may expect that as the background dimension increases the subjective difference between the options will decrease and the discrimination performance will also decrease as a result [@shafir_comparative_2014]. The same prediction can be made if we assume that the strength of preference increases under lean environmental conditions, i.e. at low reward volume or probability [@schuck-paim_state-dependent_2004]. In order to test whether the two reward dimensions (volume and probability) interact with each other even when one of them is irrelevant (being the same across choice options), we performed experiment 3.  
  
(ref:labresexp3) **Slope estimates for the effect of the background dimension on the discrimination performance in the relevant dimension**. The two choice options always differed along the relevant dimension (either probability or volume, given on the abscissa) at a fixed relative intensity. The discrimination performance for each mouse was measured at four different levels of the background dimension, which was set at the same values on both choice options during a single drinking session, but differed from condition to condition (Table \@ref(tab:conds)). Each symbol is the individual slope estimate over the four different background dimensions, color-coded for cohort number. The smallest effect size of interest (dashed lines) was determined to be the slope that would have resulted in a difference in discrimination performance of 0.1, from the lowest to the highest level of the background dimension. Large blue symbols give the means and the blue vertical lines the 90%-confidence intervals from non-parametric bootstraps.     
  
```{r exp3, fig.cap="(ref:labresexp3)"}

# slopes <- exp3 %>%
#   split(.$dim) %>%
#   map(~ lmer(performance ~ bg_level + (bg_level|IdLabel), data = .)) %>%
#   map(coef) %>%
#   map("IdLabel") %>%
#   map_df(~rownames_to_column(., var = "IdLabel"))

# exp3 %>%
#   filter(dim == "P", bg_level < 1) %>%
#   ggplot(aes(bg_level, performance, color = IdLabel)) + geom_point() +
#   stat_smooth(method = lm, se = FALSE) + facet_grid(. ~ cohort)

cohorts <- exp3 %>%
  ungroup() %>%
  select(IdLabel, cohort) %>%
  distinct()

slopes <- exp3 %>%
  group_by(IdLabel) %>%
  group_modify(~tidy(lm(performance ~ bg_level*dim, data = .))) %>%
  mutate_if(is.numeric, ~round(., 4)) %>%
  select(IdLabel, term, estimate) %>%
  spread(term, estimate) %>%
  rename(int_prob = `(Intercept)`, prob = bg_level) %>%
  mutate(vol = prob + `bg_level:dimV`,
         int_vol = int_prob + dimV) %>%
  select(-contains("dimV")) %>%
  gather(dim, estimate, -IdLabel) %>%
  mutate(type = ifelse(str_detect(dim, "_"), "intercept", "slope"),
         dim = str_replace(dim, "int_", "")) %>%
  spread(type, estimate) %>%
  inner_join(cohorts)

# slopes %>%
#   unnest(tidied) %>%
#   bind_cols(slps %>%
#   unnest(confint) %>%
#     select(-IdLabel)) %>%
#   mutate_if(is.numeric, ~round(., 4)) %>%
#   filter(str_detect(term, "bg_level")) %>%
#   ggplot() + geom_pointrange(aes(x = IdLabel, y = estimate, ymin = conf.low, ymax = conf.high)) +
#   facet_grid(. ~ term) +
#   geom_hline(yintercept = 0, linetype = 3) +
#   geom_hline(yintercept = sl_sesoi, linetype = 2) +
#   geom_hline(yintercept = -sl_sesoi, linetype = 2)


sl_sesoi <- data.frame(val = c(4/20, 20/20),
                       performance = c(0.5, 0.6)) %>%
  lm(performance ~ val, data = .) %>%
  coef() %>%
  .[2]

set.seed(42)
CIs <- slopes %>%
  group_by(dim) %>%
  group_modify(~mean_cl_boot(.x$slope, conf.int = 1 - 2*(alpha_1)))

CIs_NHT <- slopes %>%
  group_by(dim) %>%
  group_modify(~mean_cl_boot(.x$slope, conf.int = 1 - alpha_1))

slopes %>%
  ggplot() +
  geom_pointrange(data = CIs, aes(x = dim, y = y, ymin = ymin, ymax = ymax),
                  color = "darkblue", size = 1) +
  geom_hline(yintercept = sl_sesoi, linetype = 2) +
  geom_hline(yintercept = -sl_sesoi, linetype = 2) +
  theme_bw() +
  geom_jitter(aes(dim, slope, color = cohort), alpha = 0.7, width = 0.1) +
  scale_color_viridis_d() +
  labs(x = "relevant dimension", y = "slope estimate")

prob_slope <- CIs %>%
  filter(dim == "prob") %>%
  pull(ymin) %>%
  round(3)
 
mean_PV1 <- exp3 %>%
  filter(cond == "PV1") %>%
  summarise(performance = mean(performance)) %>%
  pull(performance)

pred_PV1 <-  slopes %>%
  filter(dim == "prob") %>%
  summarise(performance = mean(intercept) + prob_slope * 0.2) #bg_level = 0.2

pred_PV4 <- slopes %>%
  filter(dim == "prob") %>%
  summarise(performance = mean(intercept) + prob_slope * 1) #bg_level = 1
# 
# miceChoices %>%
#   filter(experiment == 3, !is.na(prof), cond != "training") %>%
#   mutate(prof = factor(prof)) %>%
#   group_by(cond, IdLabel, prof) %>%
#   summarise(mean_vol = mean(vol),
#             var_vol = var(vol)) %>%
# ggplot() + stat_summary(aes(cond, mean_vol, color = prof), fun.data = mean_cl_boot,
#                fun.args = list(conf.int = 0.95))

```
  
The conditions in experiment 3 were chosen to be similar to the background conditions in the previous experiments, by having one background and one relevant dimension (Table \@ref(tab:conds)). The relevant dimension always differed between the two options. For the probability dimension, we selected the same values of 0.2 and 0.5 (rel.int. = 0.86), as in the previous experiments. For the volume dimension we selected the values of 4 $\mu$L (4.8 $\mu$L in cohort 2) and 10 $\mu$L (9.6 $\mu$L in cohort 2), because the combination of a higher volume with a probability of 0.8 was expected to result in an insufficient number of visits for analysis (Table \@ref(tab:conds)). Cohort 2 had different reward volumes due to differences in the pumping process (Animals, Materials, and Methods), which also resulted in a lower relative intensity for volume (0.67 instead of 0.86). There were four different levels for each background dimension (volume and probability, Table \@ref(tab:conds)). Each mouse had its own pseudo-random sequence of the eight possible conditions. In order to test whether the background dimension affected discrimination performance, we fitted linear mixed models for each dimension, with discrimination performance as the dependent variable, background level as the independent variable and mouse as a random variable, using lme4 in R [@bates_fitting_2015]. The background level was the proportion of the actual value to the maximum of the four values tested, e.g. the background levels for volumes `r bg_vol` were `r bg_lev_vol`, respectively. We defined *a priori* a smallest effect size of interest (sesoi), as `r sl_sesoi`, which is the slope that would result from a difference of 0.1 in discrimination performance between the smallest and the largest background levels (PV1 and PV4, 0.2 and 1, respectively). A slope (whether positive or negative) within the sesoi interval was considered equivalent to zero and demonstrating a lack of an effect of background dimension.  
  
The results of experiment 3 show that the discrimination performance for volume was independent from probability as the background dimension, since the slope was equivalent to zero (Fig. \@ref(fig:resoverview), Fig. \@ref(fig:exp3)). However, the discrimination performance for probability decreased with increasing volumes, although the effect size was small (Fig. \@ref(fig:resoverview), Fig. \@ref(fig:exp3)). These results partially support the hypothesis that decision-makers may ignore reward dimensions if options do not vary along them.

## Experiment 4  
In previous experiments [@rivalan_principles_2017], mice showed an improved discrimination performance for volume over time. A potential explanation is that, with experience mice become more attuned to the relevant dimension.  [More references and explanation on selective attention in multi-dimensional choice here?]

In order to test whether the discrimination performance for one or both dimensions improves over time, we performed experiment 4, which had the same conditions as experiment 1, but with a new pseudo-random order. The same mice participated in all experiments (1-4), with about seven weeks between experiment 1 and experiment 4. We tested the discrimination performance of all mice in each experimental condition for equivalence (Table \@ref(tab:conds)). As in the previous experiments, we also used equivalence tests on the contrasts between the baselines and the congruent and incongruent conditions.  

(ref:labexp4a) **Difference in discrimination performance between identical conditions in experiment 1 and experiment 4**. Same notation as in in Fig. \@ref(fig:exp1), but with 98% confidence intervals. The sequence of conditions was pseudo-random in each experiment and different for each individual. Positive differences indicate an increase in discrimination performance with time. Mice were seven weeks old at the beginning of experiment 1 and 13-14 weeks old at the beginning of experiment 4. The discrimination performance in the incongruent condition was calculated as the relative preference for the higher volume dispenser.    
  
```{r exp4a, fig.cap="(ref:labexp4a)"}
alpha_14 <- 0.05/5 # alpha corrected for multiple comparisons (n - 1) conditions

exp14 <- summaries %>%
  filter(!str_detect(cond, "[r]"), experiment == 1 | experiment == 4) %>%
  mutate(rep = ifelse(experiment == 1, 1, 2)) %>%
  ungroup() %>%
  mutate(rep = factor(rep))

exp14_simpl <- exp14 %>%
  ungroup() %>%
  # mutate(cond = as.factor(ifelse(str_length(cond) == 4, str_sub(cond, 1, 2),
  #                                as.character(cond)))) %>%
  group_by(cond, IdLabel, cohort, rep) %>%
  summarise(performance = mean(performance))

exp14_diff <- exp14_simpl %>%
  spread(rep, performance) %>%
  mutate(difference = `2` - `1`) %>%
  droplevels()

set.seed(42)
CIs_14 <- exp14_diff %>%
  group_by(cond) %>%
  group_modify(~ mean_cl_boot(.x$difference, conf.int = 1 - 2*(alpha_14)))

CIs_14_NHT <- exp14_diff %>%
  group_by(cond) %>%
  group_modify(~ mean_cl_boot(.x$difference, conf.int = 1 - alpha_14))

exp14_diff %>%
  ggplot() +
  geom_hline(yintercept = 0.1, linetype = 2) + geom_hline(yintercept = -0.1, linetype = 2) +
  geom_pointrange(data = CIs_14, aes(x = cond, y = y, ymin = ymin, ymax = ymax),
                  color = "darkblue", size = 1) +
  theme_bw() + geom_jitter(aes(cond, difference, color = cohort), alpha = 0.7, width = 0.1) +
  ylab("difference (after - before)") +
  scale_color_viridis_d()

```
  
In the comparison between experiment 1 and experiment 4, mice showed an improved discrimination performance in both volume baselines and in the incongruent condition (Fig. \@ref(fig:exp4a)). There was no change in the BVP2 and C conditions, and the results for BPV1 were inconclusive. Thus, consistent with our prior findings, mice improved their volume discrimination over time. The discrimination performance in the congruent condition was better than in the probability baseline, but the same as in the volume baseline (Fig. \@ref(fig:exp4b)). The discrimination in the incongruent condition was lower than in any of the two baselines, but the difference to the volume baseline was smaller (Fig. \@ref(fig:exp4b)). Thus, as in our previous experiments, mice showed an improvement in volume discrimination over time. Furthermore, compared to experiment 1 the influence of the volume dimension on choice was even more pronounced.  

(ref:labexp4b) **Difference between discrimination performance in the baseline conditions and in the congruent and incongruent conditions in experiment 4**. Same notation as in Fig. \@ref(fig:exp1). The discrimination performance in the incongruent condition was calculated as the relative preference for the higher probability dispenser when comparing to the probability baseline and for the higher volume dispenser when comparing to the volume baseline. Compare to Fig. \@ref(fig:exp1). 

```{r exp4b, fig.cap="(ref:labexp4b)"}
exp4_simpl <- exp14_simpl %>%
  filter(rep == 2) %>%
  ungroup() %>%
  mutate(cond = as.factor(ifelse(str_length(cond) == 4, str_sub(cond, 1, 2),
                                 as.character(cond)))) %>%
  group_by(cond, IdLabel, cohort) %>%
  summarise(performance = mean(performance))

exp4_diff <- exp4_simpl %>%
  spread(cond, performance) %>%
  mutate("BP - BV" = BP - BV, "C - BP" = C - BP, "C - BV" = C - BV,
         "I - BP" = 1 - I - BP, "I - BV" = I - BV) %>%
  select(-BV, -BP, -C, -I) %>%
  gather(cond, difference, -cohort, -IdLabel)

set.seed(42)
exp4_diff %>%
  filter(cond != "BP - BV") %>%
  ggplot(aes(cond, difference)) +
  geom_hline(yintercept = sesoi, linetype = 2) + geom_hline(yintercept = -sesoi, linetype = 2) +
  stat_summary(aes(cond, difference), fun.data = mean_cl_boot,
               fun.args = list(conf.int = 1 - 2*(alpha_2)),
               color = "darkblue", size = 1) +
  theme_bw() + geom_jitter(aes(color = cohort), alpha = 0.7, width = 0.1) +
  xlab("") + scale_color_viridis_d()
```


## Decision models of two-dimensional choice

We based our decision models on the Scalar Utility Theory (SUT, @kacelnik_risky_1998; @rosenstrom_scalar_2016), which models memory traces for reward amounts as normal distributions rather than point estimates. The scalar property is implemented by setting the standard deviations of these distributions to be proportional to their means. Choice between two options with different amounts can be modelled by taking a single sample from each memory trace distribution and selecting the option with the larger sample.   
As previously explained, the discrimination performance of reward probability can be reasonably predicted by the relative intensity of the two options [@rivalan_principles_2017]. This suggests that the memory traces of reward probabilities also exhibit the scalar property, so that discrimination of small probabilities (e.g. 0.2 vs. 0.5, rel.int. = 0.86) is easier  than discrimination of large probabilities (e.g. 0.5 vs. 0.8, rel.int. = 0.46). Consequently, discrimination (of either volumes or probabilities) when options vary along a single dimension can be modelled by SUT.  
In order to extend the model for multi-dimensional choice situations, we implemented six variations that differed in the integration of information from the volume and probability dimensions (Table \@ref(tab:modTable)). The information from the different reward dimensions was used to obtain for each choice option a *remembered value* (utility), which exhibited the scalar property. Choice was simulated by single sampling from the remembered value distributions.  

|model | description | remembered value | criterion |
|------|-------------|------------------|-----------|
|1 | scalar expected value | $\pi N(v,\gamma \cdot v)$ | - |
|2 | hurdle                | $N(\pi, \gamma \cdot \pi)\times N(v, \gamma \cdot v)$ | - |
|3 | random dimension      | $N(r, \gamma \cdot r)$ | $\theta$ = 0.5 |
|4 | winner-takes-all      | $N(r, \gamma \cdot r)$ | $\theta$ = 1 |
|5 | probability first     | $N(r, \gamma \cdot r)$| if $s(\pi) > 0.8$ then $r = \pi$, if $s(v) > 0.8$ then $r = v$, otherwise $\theta$ = 0.5 |
|6 | volume first          | $N(r, \gamma \cdot r)$| if $s(v) > 0.8$ then $r = v$ , if  $s(\pi) > 0.8$ then $r = \pi$ , otherwise $\theta$ = 0.5 |
  Table: (\#tab:modTable) Decision-making models. 
    
  Note: $v$ - volume estimate; $\pi$ - probability estimate; $\gamma$ - coefficient of variation; $r$ - either $v$ or $\pi$ depending on the *criterion*; $\theta$ - probability of selecting the dimension with the higher salience; $s(r)$ - salience of dimension $r$, calculated as $\frac{max(r) - min(r)}{\overline{r}}$, where $n$ is the number of options, and $\overline{r}$ is the average of $r$ over all options. 

```{r}

#### gamma baseline fit for probability

sut_fit <- function(phys1, phys2, gamma, beta) {
  pnorm(0, mean = phys1 - phys2, sd = beta + gamma * sqrt(phys1^2 + phys2^2), lower.tail = FALSE)
}

# basel <- summaries %>%
#   filter(str_detect(cond, "BPV1"), experiment != 2) %>%
#   mutate(phys1 = ifelse(experiment == 2, 1, 0.5),
#          phys2 = 0.2) %>%
#   group_by(IdLabel) %>%
#     mutate(lapse = mean(sampling) * 2,
#            relev_perf = 1 - sampling)
# 
# basel %>%
#   mutate(experiment = factor(experiment)) %>%
#   ggplot(aes(performance, fill = cohort)) + geom_density(alpha = 0.2)

baselines <- summaries %>%
  filter(str_detect(cond, "BP"), experiment != 2) %>%
  mutate(phys1 = ifelse(experiment == 2, 1, 0.5),
         phys2 = 0.2) %>%
  group_by(IdLabel) %>%
    mutate(lapse = mean(sampling) * 2,
           relev_perf = 1 - sampling)

gammas <- baselines %>%
  group_by(IdLabel, cohort) %>%
  group_modify(~tidy(coef(nls(performance ~ sut_fit(phys1, phys2, gamma, beta = 0),
                                     weights = Nrel,
                                     start = c(gamma = 0.5),
                                     lower = c(gamma = 0),
                                     upper = c(gamma = 10),
                                     data = .x,
                                     algorithm = "port",
                                     control = list(maxiter = 50000,
                                                    minFactor = 1/2000))))) %>%
  select(gamma = x, -names)

gamma <- gammas %>%
  ungroup() %>%
  summarise(median = median(gamma),
            mad = mad(gamma)) %>%
  mutate_if(is.numeric, round, 2)

# cvs <-  baselines %>%
#   nest(-IdLabel) %>%
#   mutate(test = map(data, ~ coef(nls(performance ~ cv_fit(phys1, phys2, cv, lapse = 0), 
#                                 weights = Nrel,
#                                 start = c(cv = 0.5),
#                                 lower = c(cv = 0),
#                                 upper = c(cv = 5),
#                                 data = .x,
#                                 algorithm = "port",
#                                 control = list(maxiter = 5000))))) %>%
#   unnest(test) %>%
#   unnest(data) %>%
#   group_by(IdLabel, cohort, lapse) %>%
#   summarise(cv = mean(test))
# 
# mean_cvs <- cvs %>%
#   ungroup() %>%
#   summarise_at(vars(cv, lapse), list(mean, se)) %>%
#   mutate_if(is.numeric, round, 2)

baselines_vol <- summaries %>%
  filter(str_detect(cond, "BV")) %>%
  mutate(phys1 = 20,
         phys2 = 4) %>%
  group_by(IdLabel) %>%
  mutate(lapse = mean(sampling) * 2,
           relev_perf = 1 - sampling)

gammas_vol <- baselines_vol %>%
  group_by(IdLabel, cohort) %>%
  group_modify(~tidy(coef(nls(performance ~ sut_fit(phys1, phys2, gamma, beta = 0),
                                     weights = Nrel,
                                     start = c(gamma = 0.5),
                                     lower = c(gamma = 0),
                                     upper = c(gamma = 10),
                                     data = .x,
                                     algorithm = "port",
                                     control = list(maxiter = 50000,
                                                    minFactor = 1/2000))))) %>%
  select(gamma = x, -names)

median_gamma_vol <- gammas_vol %>%
   group_by(cohort != 2) %>%
   summarise(med_gamma = median(gamma), mad_gamma = mad(gamma))


sims_data <- read.csv2(file = "data/simulations_coh2.csv",
                        header = TRUE, dec = ".", sep = ";", na.strings = "NA")

summ_sims <- sims_data %>%
  group_by(experiment, cond, model) %>%
  summarise(m_perf = median(performance))

summ_sims <- summ_sims %>%
  filter(experiment == 1) %>%
  ungroup() %>%
  mutate(experiment = 4) %>%
  bind_rows(summ_sims) %>%
  arrange(experiment, cond, model)

emp_perf <- summaries %>%
  filter(!str_detect(cond, "[r]")) %>%
  select(IdLabel, cohort, experiment, cond, performance)

emp_perf <- emp_perf %>%
  filter(cond == "BVP1") %>%
  mutate(experiment = 2) %>%
  bind_rows(emp_perf) %>%
  arrange(IdLabel, cohort, cond)

devs <- summ_sims %>%
  full_join(emp_perf) %>%
  mutate(deviance = (m_perf - performance)^2,
         dev = abs(m_perf - performance))


RMSEs <- devs %>%
  filter(!(str_detect(cond, "BP") & experiment != 2)) %>%
  group_by(experiment, model) %>%
  summarise(RMSE = sqrt(mean(deviance, na.rm = TRUE))) %>%
  arrange(experiment, RMSE)

rmse_ranks <- function(tibb) {
  tibb %>%
    mutate(rank = rank(RMSE)) %>%
    ungroup() %>%
    select(-RMSE) %>%
    spread(experiment, model)
}

ranks <- RMSEs %>%
  rmse_ranks()

RMSEs_cohorts <- devs %>%
  filter(!str_detect(cond, "BP")) %>%
  mutate(cage1 = cohort != 2) %>%
  group_by(experiment, model, cage1) %>%
  summarise(RMSE = sqrt(mean(deviance, na.rm = TRUE))) %>%
  arrange(cage1, experiment, RMSE)

ranks_coh2 <- RMSEs_cohorts %>%
  filter(!cage1) %>%
  select(-cage1) %>%
  group_by(experiment) %>%
  rmse_ranks()

ranks_coh13 <- RMSEs_cohorts %>%
  filter(cage1) %>%
  select(-cage1) %>%
  group_by(experiment) %>%
  rmse_ranks()
```

```{r simRankTable}
ranks %>%
  knitr::kable(booktabs = TRUE,
               caption = "Best performing models ranked by root-mean-square-errors (RMSE).") %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"),
                full_width = T) %>%
  kableExtra::add_header_above(c(" " = 1, "experiment" = 4))
```

There was no single model that could best explain the choice of the mice in all experiments, but models 1 (scalar expected value), 2 (hurdle), and 4 (winner-takes-all) were in the top-three performing models for three out of the four experiments (Tables \@ref(tab:modTable), \@ref(tab:simRankTable), see also Appendix 1 Figures \@ref(fig:simexp1), \@ref(fig:simexp2), \@ref(fig:simexp3), and \@ref(fig:simexp4)). However, due to the striking differences in performance between cohort 2 and the other cohorts, we also ranked the models separately for the different mouse groups, depending on which cage they performed the experiments in (cohorts 1 and 3 in cage 1 and cohort 2 in cage 2). Indeed, two different patterns emerged for the different cages. For the two cohorts in cage 1, models 1 (scalar expected value) and 4 (winner-takes-all) were the best supported models, followed by 2 (hurdle) and 6 (volume first). Notably, model 6 (volume first) was the best performing model in experiments 3 and 4, but the worst-performing model in experiments 1 and 2. In contrast, model 5 (probability first) was the best supported model for cohort 2 in all experiments, followed by models 3 (random dimension) and 2 (hurdle).

```{r coh13RankTable}
ranks_coh13 %>%
  knitr::kable(booktabs = TRUE,
               caption = "Best performing models ranked by root-mean-square-errors (RMSE) for cohorts 1 and 3.") %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"),
                full_width = T) %>%
  kableExtra::add_header_above(c(" " = 1, "experiment" = 4))
```


```{r coh2RankTable}
ranks_coh2 %>%
  knitr::kable(booktabs = TRUE,
               caption = "Best performing models ranked by root-mean-square-errors (RMSE) for cohort 2.") %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"),
                full_width = T) %>%
  kableExtra::add_header_above(c(" " = 1, "experiment" = 4))
```
# Discussion

The foraging choices of the mice in this study provide evidence both for and against the full use of information. In the first two experiments, mice showed different discrimination performances in the conditions in which both reward dimensions were relevant (congruent and incongruent conditions) compared to the baselines, in which only one of the two dimensions was relevant (Figs. \@ref(fig:exp1), \@ref(fig:exp2)). Consequently, the best supported models for these experiments (here we exclude cohort 2 and address the difference between cohorts below) were the models that made use of the full information from both reward dimensions, or from the dimension that was subjectively more salient (Table. \@ref(tab:coh13RankTable)). Although these models were good predictors of the mouse choices in experiments 3 and 4 as well, the best-performing model was the one that only considered the probability dimension if differences on the volume dimension were insufficient to reach a decision (Fig. \@ref(fig:exp4b), Table \@ref(tab:coh13RankTable)). Thus, it appears that mice initially used information from all reward dimensions without bias, but with experience started to rely more on one reward dimension and disregard the other when both dimensions differed between choice options.  
In similar and more complex choice situations when options vary on several dimensions, an animal has no immediate method of distinguishing the relevant from the background dimensions. Instead it has to rely on its experience over many visits before it can obtain information about the long-term profitability associated with the different reward dimensions. Under such circumstances a decision rule that considers all or the most salient reward dimensions initially and prioritizes dimensions based on gathered experience can be profitable without being too computationally demanding. Indeed, with the particular experimental design in this study, a mouse using a "volume first" priority heuristic would have preferrentialy visited the more profitable option (whenever there was one) in every single experimental condition, including the incongruent conditions.  

## Scalar property considerations

For laboratory mice, which usually have unrestricted access to a water bottle, the volume of a water reward is not a stimulus that predicts reward profitability. An alternative explanation of our results is that the mice used the "volume first" heuristic from the beginning of the experiment, but only became better at discriminating volumes in the last two experiments. This interpretation is supported by the comparison between experiments 1 and 4 (Fig. \@ref(fig:exp4a)), as well as from previous experiments [@rivalan_principles_2017], in which mice improved their volume discrimination over time. It is not possible with these data to distinguish whether the effect was caused by training or age. Perhaps an increase in mouth capacity or in the number of salt taste receptors due to aging [check with literature] allows adult mice to better discriminate water volumes. Comparing the discrimination performance of older naive and younger trained mice would help clarify this confound. In any case, the change in discrimination performance for volume suggests that the scalar property only approximately holds, and that the $\gamma$ for volume is not truly constant over a long period of time. This can be seen as evidence against the scalar expected value model and support for two different scalars ($\gamma_{\pi} \neq \gamma_{v}$) in the hurdle model. Alternatively, there is only one scalar, but the weights of the two dimensions change over time (as in a biased random dimension model with unequal dimension weights). Yet another model extension that can account for the observed discrimination performances would be to introduce an explicit sampling (exploration-exploitation balance) method [@nachev_behavioral_2019; @sih_linking_2012]. With this implementation there is no change in the scalar property, but the frequency of sampling visits changes over time. The biggest challenge is that, in contrast to time intervals for which the peak procedure exists [@kacelnik_risky_1998], we do not have a method to interrogate the probability or volume estimate that an animal has, in order to more directly measure the scalar factor rather than infer it from choice behavior.  

```{r}
n_vis_exp3 <- miceChoices %>%
  filter(experiment == 3) %>%
  mutate(cohort = factor(cohort)) %>%
  group_by(day, cond, IdLabel, cohort) %>%
  summarise(count = max(count),
            relcount = max(relcount)) %>%
  inner_join(exp3_tab %>%
  select(cond, bg_level, cohort, dim))

# n_vis_exp3 %>%
#   ggplot() +
#   geom_point(aes(bg_level, relcount, color = IdLabel)) +
#     stat_smooth(aes(bg_level, relcount, color = IdLabel), method = lm, se = FALSE) +
#   facet_grid(cohort ~ dim)

n_vis_exp3 <- n_vis_exp3 %>%
  group_by(dim, cond) %>%
  summarise(mean_n = round(mean(relcount), 0),
            sd = round(sd(relcount, na.rm = TRUE), 0))

n_low <- n_vis_exp3 %>%
  filter(cond == "PV1") %>%
  select(-dim, -cond) 

n_high <- n_vis_exp3 %>%
  filter(cond == "PV4") %>%
  select(-dim, -cond)

```
  
## Interaction between dimensions and comparative reward evaluation  
  
Although mice were about equally good at discriminating volume rewards at each different probability, the discrimination of probabilities decreased at higher volumes (Fig. \@ref(fig:exp3); the estimated effect size was a decrease of `r mean(pred_PV1$performance - pred_PV4$performance)` between a volume background at 4 $\mu L$ and at 20 $\mu L$). This suggests that the two dimensions interact with each other. Absolute reward evaluation [@shafir_intransitivity_1994; @shafir_comparative_2014] and state-dependent evaluation [@schuck-paim_state-dependent_2004] are both consistent with this decrease in discrimination performance, but not with the lack of effect in the conditions in which the probability was the background dimension. With comparable return rates (Table \@ref(tab:conds)) between the two series of conditions, these hypotheses make the same predictions regardless of which dimension is relevant and which is background. An alternative explanation is that arriving at a good estimate of probability requires a large number of visits and when the rewards are richer (of higher volume), mice satiate earlier and make a smaller total number of visits, resulting in poor estimates of the probabilities and poorer discrimination performance. Consistent with this explanation, mice made on average ($\pm$ SD) `r paste(n_low$mean_n, "±", n_low$sd)` nose pokes at the relevant dispensers at 4 $\mu L$, but only `r paste(n_high$mean_n , "±", n_high$sd)` nose pokes at 20 $\mu L$ (Fig. \@ref(fig:npokes)).  

As mentioned earlier, researchers have proposed that with absolute reward evaluation the difference/mean ratio in an experimental series like our experiment 3 should decrease with the increase of the background dimension, leading to a decrease in the proportion preference for the high-profitability alternative (i.e. discrimination performance) [@shafir_comparative_2014]. However, if we multiply the estimates for each dimension together, as in our models 1 and 2, we have an implementation that qualifies as absolute reward evaluation (because a single utility is calculated from the different reward dimensions), which however does not satisfy this prediction (Fig. \@ref(fig:exp3models)). In fact, none of our models exhibited an effect of the background dimension in experiment 3 on the discrimination performance, with all slopes equivalent to zero (Fig. \@ref(fig:exp3models)). Thus, our results also show that the background dimensions need not have an effect on the discrimination performance, even with absolute reward evaluation.  

## Difference between cohorts

(ref:Idurs) **Visit durations during rewarded and unrewarded nose pokes for the three cohorts in all experiments.** Columns give the status of the nose poke (rewarded or unrewarded) and rows, the experiment number (1-4).  Data from the three cohorts are represented by differently color-filled density curves from the observed individual nose poke durations. Note the logarithmic scale on the abscissa.    

```{r Idurs, fig.height = 7, fig.width = 10, fig.cap="(ref:Idurs)"}

prop_long <- miceChoices %>%
  mutate(long = ifelse(eventDuration > 600000, 1, 0)) %>%
  summarise(prop = round(mean(long), 4)) %>%
  pull()
  

miceChoices %>%
  # filter(eventDuration < 900000) %>%
  mutate(log_dur = log(eventDuration),
         cohort = factor(cohort),
         rewardstatus = ifelse(rewardstatus > 0, "rewarded", "unrewarded")) %>%
  ggplot(aes(eventDuration/1000, fill = cohort)) +
  geom_density(alpha = 0.2) +
  facet_grid(experiment ~ rewardstatus) +
  xlab("duration of nose poke [s]") +
  scale_x_log10() +
  coord_cartesian(xlim = c(0.1, 100)) +
  theme_bw() +
  scale_fill_viridis_d()

```

The most likely explanation that we can give for the unexpected difference in behavior between the cohorts (most obvious in Fig. \@ref(fig:exp4b)) is a cage effect. As explained in Animals, Methods, and Materials, the precision of the reward volumes was lower in cage 2, which housed cohort 2. It is unlikely that such a small magnitude of the difference ($0.33 \pm 0.03$  $\mu Lstep^{-1}$ in cage 1 vs. $1.56 \pm 0.24$  $\mu Lstep^{-1}$ in cage 2) could influence volume discrimination to the observed extent. Future experiments can address this issue by specifically manipulating the reliability of the volume dimension using the higher-precision pump. However, we suspect that the difference between cohorts might have been caused by the acoustic noise and vibrations produced by the stepping motors of the pumps. The pump in cage 1 was much louder, whereas the one in cage 2 was barely audible (to a human experimenter). This could have made it harder for mice in the soft cage (cage 2) to discern whether a reward was forthcoming. As a result, mice waited longer before leaving the dispenser during unrewarded nose pokes  (Fig. \@ref(fig:Idurs)). This potentially costly delay might have increased the relative importance of the probability dimension, resulting in the observed discrimination performance in cohort 2. Furthermore, the same line of reasoning can also explain the improved volume discrimination form the first to the fourth experiment: there was a shift towards shorter visit durations in the cohorts in the loud cage (cage 1) from the first to the fourth experiment (Fig. \@ref(fig:Idurs)), suggesting that mice had learned over time to abort the unrewarded visits. In an unrelated experiment we tested two cohorts of mice in both cages simultaneously and then translocated them to the other cage. The results demonstrated that differences in discrimination performance were primarily influenced by cage and not by cohort (data not shown). Thus, the sound cue associated with reward delivery may be an important confounding factor in probability discrimination in mice, as it provides a signal for the reward outcome [@ojeda_paradoxical_2018].  

# Animals, Methods, and Materials

## Subjects

The experiments were conducted with C57BL/6NCrl mice (Charles River, Sulzfeld, Germany, N_total_ = 30). Mice were five weeks old on arrival. The mice from each cohort were housed together. They were marked with unique Radiofrequency Identification tags (RFID: 12 × 2.1 mm, 125 kHz, Sokymat, Rastede, Germany) and also earmarked at age six weeks. At age seven weeks mice were transferred to the automated group home cage for the main experiment. Pellet chow (V1535, maintenance food, ssniff, Soest, Germany) was always accessible from a trough in the cage lid. Water was available from the operant modules of the automated group cage, depending on individual reward schedules. Light conditions in the experiments were 12:12 LD and climatic conditions were 23 $\pm$ 2 $^\circ$C and 50–70% humidity.

## Ethics statement

The experimental procedures were aimed at maximizing animal welfare. During experiments, mice remained undisturbed in their home cage. Data collection was automated, with animals voluntarily visiting waters dispensers to drink. The water intake and health of the mice was monitored daily. Due to the observational nature of the study, animals were free from damage, pain, and suffering. The animals were not sacrificed at the end of the study, which was performed under the supervision and with the approval of the animal welfare officer (*Tierschutzbeauftragter*) heading the animal welfare committee at the Humbold University. Experiments followed national regulations in accordance with the European Communities Council Directive 10/63/EU.  

## Cage and dispenser system

We used automated cages (612 × 435 × 216 mm, P2000, Tecniplast, Buggugiate, Italy) with woodchip bedding (AB 6, AsBe-wood, Gransee, Germany), and enriched with two grey PVC tubes and paper towels as nesting material. The cage was outfitted with four computer-controlled liquid dispensers. The experimental set-up is described in detail in @rivalan_principles_2017. Briefly, mice were detected at the dispensers via infrared beam-break sensors and RFID-sensors. Water delivery at each dispenser could be controlled, so that it could be restricted or dispensed at different amounts on an individual basis. Mice were therefore rewarded with droplets of water from the dispenser spout that they could remove by licking. We changed  cage bedding and weighed all animals on a weekly basis, always during the light phase and at least an hour before the start of the testing session. Data were recorded  and stored automatically on a laptop computer running a custom-written software in C#, based on the .NET framework. Time-stamped nose poke events and amounts of water delivered were recorded for each dispenser, with the corresponding mouse identity.  
A second automated group cage (cage 2) was made for the purposes of this study, nearly identical to the one described above (cage 1). The crucial modification was that the stepping-motor syringe pump was replaced with a model that used disposable plastic 25-mL syringes instead of gas-tight Hamilton glass syringes (Series 1025). Thus, the pumping systems in the two cages differed in the smallest reward that could be delivered and in the precision of reward delivery (mean $\pm$ SD: $0.33 \pm 0.03$  $\mu Lstep^{-1}$ in cage 1 vs. $1.56 \pm 0.24$  $\mu Lstep^{-1}$ in cage 2). The precision of each pump was estimated by manually triggering reward visits at different preset pump steps (17 and 42 in cage 1, 3 and 12 in cage 2) and collecting the expelled liquid in a graduated glass pipette placed horizontally next to the cage. Each dispenser was measured at least 20 times for each pump step value.  

## Experimental schedule

The general experimental procedure was the same as in @rivalan_principles_2017. 
The water dispensers were only active during a 18h-long drinking session each day, which began with the onset of the dark phase and ended six hours after the end of the dark phase. The reward properties (volume and probability) were dependent on the experimental condition. Although individual mice shared the same dispensers inside the same cage, they were not necessarily in the same experimental phase or experimental condition. The three cohorts (1-3 in chronological order) were tested consecutively, with cohort 2 housed in cage 2 and the other cohorts housed in cage 1. If after any drinking session in any experimental phase a mouse drank less than 1000 $\mu L$ of water, we placed two water bottles in the automated cage for 10-15 min, awakened all mice and allowed them to drink freely until they voluntarily stopped.  

### Exploratory phase

At the beginning of this phase there were ten mice in each cohort, except for cohort 2, in which one mouse was excluded due to failed transponder implantation (the mouse was in good health condition). The remaining mice were transferred to the automated cages 1-2 hours before the first drinking session of the exploratory phase. The purpose of this phase was to let mice accustom to the cage and learn to use the dispensers to obtain water. Therefore, each nose poke at any dispenser was rewarded with a constant volume of 20 $\mu L$. The criterion for advancing to the following training phase was consuming more than 1000 $\mu L$ in a single drinking session. Mice that did not reach the criterion remained in the exploratory phase until they either advanced to the following phase or were excluded from the experiment.   

### Training phase

In this phase the reward volume was reduced to 10 $\mu L$ and the reward probability was reduced to 0.3 at all dispensers. These reward values ensured that mice remained motivated to make several hundred visits. The training phase was repeated for two to three days until at least eight mice fulfilled the criterion of consuming more than 1000 L of water in one drinking session. The purpose of the training phase was to introduce mice to the reward dimensions (volume and probability) that would be used in the following discrimination experiments. In cohorts 1 and 2, mice were excluded from the experiment if they did not reach the criterion in two days, or,  alternatively, if more than eight mice had reached the criterion, mice were excluded at random to ensure a balanced number of mice per dispenser.  

### Autoshaping phase

We introduced an autoshaping phase for the mice in cohort 3, because after two days only six of them had advanced to the training phase. The unusually low number of visits made by mice that did not pass the exploratory phase suggested that the noise produced by the pumping systems might scare naive, shy mice away from the dispensers. In order to ensure that all mice were succesfully trained, we designed the autoshaping phase so that rewards at all four dispensers were delivered at regular intervals (7 $\mu L$ every minute), regardless of the behavior of the mice.  After two days, all mice had made at least 200 nose pokes and the cohort was then moved to the last succesfully reached phase, either exploratory or training. Two days later all mice succesfully completed the training phase and two of the mice that last reached the criterion were randomly selected out of the experiment, bringing the number of mice to eight. We therefore updated our training procedure to always begin with the autoshaping phase, followed by the exploratory phase and the training phase.   

### General procedure in the main experiments

After eight mice had succesfully passed the training phase, they proceeded with experiment 1 from the main experiments (1-4). In all of the main experiments mice had a choice between four dispensers, where two were not rewarding and the other two gave rewards with volumes and probabilities that depended on the experimental condition (Table \@ref(tab:conds)). In most conditions one of the rewarding dispensers (high-profitability dispenser) was more profitable than the other (low-profitability dispenser). The sequence of conditions was randomized for each individual, so that any given mouse was usually experiencing a different experimental condition than all other mice. On any given day two of the dispensers were rewarding for four mice and the other two were rewarding for the other four mice. Within each group of four, each pair of mice shared the same high and low-profitability dispensers, which were spatially inverted between pairs of mice. This pairing was done to increase the throughput of the experiments, while controlling for potential social learning effects and distributing mice evenly over the dispensers to minimize crowding effects.  
As a control for positional biases, each condition was followed by a reversal on the next day, so that the high and low-profitability dispensers were spatially inverted for all mice, whereas the two non-rewarding dispensers remained unchanged. Reversal was followed by the next experimental condition, with random distribution of the dispensers among the pairs of mice following the constraints described above. Over the 50 total days in the main experiment (twice the number of conditions shown in Table \@ref(tab:conds), because of reversals, plus experiment 4), each mouse experienced each dispenser as a high-profitability dispenser between 11 and 14 times. In the event of an electrical or mechanical malfunction, data from the failed condition and its reversal were discarded and the failed condition was repeated at the end of the experiment. Such a failure occured once in cohort 1, four times in cohort 2 and did not occur in cohort 3. After experiments 1 and 2, mice were given another training phase (rewards with 10 $\mu L$ and 0.3 probability) for a single day, before they proceeded with the next experiment. After experiment 3 mice were given water ad libitum for four days, followed by one day in the training phase, before proceeding with experiment 4. At the end of experiment 4 mice were returned to the animal facility.  

## Data analysis

```{r}
n_pokes <- miceChoices %>%
  filter(!str_detect(cond, "[r]")) %>%
  count(cond, IdLabel, experiment, day, cohort, rel)

mean_tot_pokes <- n_pokes %>%
  group_by(cond, IdLabel, experiment, day, cohort) %>%
  summarise(Ntot = sum(n)) %>%
  ungroup() %>%
  summarise(mean = mean(Ntot), sd = sd(Ntot)) %>%
  mutate_all(funs(round(., 0)))

mean_perc_pokes <- n_pokes %>%
  group_by(cond, IdLabel, experiment, day, cohort) %>%
  mutate(Ntot = sum(n), perc = n/Ntot) %>%
  group_by(rel) %>%
  summarise(mean_perc = mean(perc), sd = sd(perc)) %>%
  mutate_all(funs(round(., 2)))

mean_perc_rel <- mean_perc_pokes %>%
  filter(rel == 1)
```

On average (mean $\pm$ SD), mice made `r paste(mean_tot_pokes$mean, "±", mean_tot_pokes$sd)` nose pokes per drinking session (Fig. \@ref(fig:npokes)), with an average proportion of `r paste(mean_perc_rel$mean, "±", mean_perc_rel$sd)` nose pokes at the rewarding dispensers. In order to focus on post-acquisition performance [@rivalan_principles_2017], we excluded the first 150 nose pokes at the rewarding dispensers. We then calculated the *discrimination performance* for each mouse and each condition of each experiment. Since each condition was repeated twice (original condition and reversal), we calculated the discrimination performance as the total number of nose pokes at the high-profitability dispenser divided by the sum of the total number of nose pokes at the high- and at the low-profitability dispensers. Nose pokes at the non-rewarding dispensers were ignored. In the conditions in which the profitability was equal, the dispenser with the higher reward volume was treated as the "high-profitability" dispenser.  
Data analysis was done using R [@r_development_core_team_r:_2019]. When comparing discrimination performances we used the two one-sidede procedure (TOST) for equivalence testing [@lauzon_easy_2009; @lakens_equivalence_2017]. First, we picked a smallest effect size of interest (sesoi) *a priori* as the difference in discrimination performance of 0.1 units in either direction. (The sesoi can be graphically represented as the [-0.1, 0.1] interval around the difference of zero.) Then, we estimated the mean differences and their confidence intervals from 1000 non-parametric bootstraps using the `smean.cl.boot` function in the package `Hmisc` [@r_Hmisc_2019]. For a single equivalence test the 90% confidence interval is usually constructed, i.e. $1 - 2\alpha$ with $\alpha = 0.05$, because both the upper and the lower confidence bounds are tested against the sesoi [@lauzon_easy_2009; @lakens_equivalence_2017]. Thus, equivalence was statistically supported if the confidence interval was completely bounded by the sesoi interval around 0 (the null hypothesis). A difference was considered to be statistically supported if the $1 - \alpha$ confidence interval did not contain zero and the $1 - 2\alpha$ confidence interval was not completely bounded by the sesoi interval. If the $1 - \alpha$ confidence interval contained zero, but the $1 - 2\alpha$ was not completely inside the sesoi interval, then results were inconclusive. We corrected for multiple comparisons by dividing $\alpha$ by *n - 1*, where *n* was the number of comparisons in each experiment [@lauzon_easy_2009], e.g. we constructed the 97% and the 98% confidence intervals in experiments 1, 2, and 4, and the 98% and 99% confidence intervals in experiment 3 and for the comparison between experiments 1 and 4.      

## Simulations

Simulations were done in R [@r_development_core_team_r:_2019]. Code is available at zenodo repository xxx. 

### Environment

Each of the experimental conditions was recreated in the simulations as a binary choice task between the high-profitability and the low-profitability options. We did not simulate the two non-rewarding options. Upon a visit by a virtual mouse, a choice option would deliver a reward with its corresponding volume and probability (Table \@ref(tab:conds_tab)). The virtual environment was not spatially and temporally explicit. Thus, no reversal conditions were simulated and the test of each experimental condition consisted in a sequence of 100 choices. All experimental conditions in all four experiments were tested.  

### Virtual mice

For simplicity and in order to simulate post-acquisition discrimination performance, we assumed that each mouse had a precise estimate of each of the two reward dimensions for both choice options. The virtual mice thus began each experimental condition in a learned state and (further) learning was not simulated.  
From its memory traces a virtual mouse generated one *remembered value* distribution for each choice option, according to one of six different rules (models, Table \@ref(tab:modTable)). Action selection was then implemented by taking a single sample from each distribution and selecting the option with the larger sample.  

### Remembered value models

All six models implemented the *scalar property* from the Scalar Utility Theory (SUT, @kacelnik_risky_1998; @rosenstrom_scalar_2016), because the remembered value was modelled as a normal distribution with a standard deviation proportional to its mean. However, the models differed in the way information from the two reward dimensions was used (either through integration or heuristics).   

These models were:  

1. *Scalar expected value model*. There is a single memory trace for each option and it consists in the simple product of the estimate for the volume and the estimate for the probability (expected value). The scalar property is implemented as $\pi N(v,\gamma \cdot v)$, where $v$ is the volume estimate, $\pi$ is the probability estimate, and $\gamma$ is a free parameter, the coefficient of variation. This model thus utilizes information from all dimensions for every decision.    

2. *Hurdle model*. There are traces for each dimension for every option, where each trace exhibits the scalar property independently and the value is obtained by simple multiplication of the traces for each dimension: $N(\pi, \gamma \cdot \pi)\times N(v, \gamma \cdot v)$. This model also utilizes information from all dimensions for every decision. Although this model allows each dimension to have its own scalar factor, e.g. $\gamma_{\pi} \neq \gamma_{v}$, for the sake of simplicity we assume that they are both equal.   

The memory traces in the remaining models are identical to the traces in model 2, but these models usually consider only a single dimension.  
  
3. *Random dimension model*. Each decision is based on a single dimension, selected with probability 0.5.  

4. *Winner-takes-all model*. Each decision is based only on the dimension with the higher subjective salience. The salience for a vector of samples $(S_i)$ from the underlying memory traces along one dimension, e.g. volume $v = (S_{1}, S_{2}, ..., S_{n})$, is calculated as $\frac{max(v) - min(v)}{\overline{v}}$, where $n$ is the number of options. In the case of $n = 2$, the salience is equivalent to the previously described relative intensity measure. For dimensions of equal salience the model reverts to model 3.  
  
The last two models are examples of a lexicographic rule, in which the dimensions are checked in a specific order. If the salience of a dimension is higher than a given threshold, then a decision is made based only on this dimension. Otherwise the next-order dimension is checked. If all dimensions have saliences below the threshold, the model reverts to model 3.  The value of the threshold was 0.8, the psychometric function threshold for probability [@rivalan_principles_2017].  

5. *Probability first model*. Probability is checked first, then volume.   

6. *Volume first model*. Volume is checked first, then probability.  

### Model fits

All models described above share the same free parameter, the scalar factor $\gamma$. In order to obtain estimates for $\gamma$, we first fitted each models to the probability baseline discrimination performances of all mice in experiments 1, and 4 (conditions BPV1 and BPV2). The resulting median estimates (across animals) for $\gamma$ were then used as the values of the free parameter in out-of-sample tests of the six models. For each of the experimental conditions in the four experiments (Table \@ref(tab:conds_tab)) and for each of the six models we simulated 100 choices by 100 (identically parametrized) mice. Over the 100 choices we calculated the discrimination performance for each mouse and then used the median of the individual discrimination performances as the model prediction. We then quantified the model fits to the empirical data by calculating root-mean-square-errors (RMSE), excluding the BPV1 and BPV2 conditions in experiments 1 and 4. Finally, we ranked the models by the RMSE scores.

\newpage
# Appendix {-}

\beginsupplement


(ref:labsimexp1) **Comparison of discrimination performances in all six simulation models and in the three mouse cohorts in Experiment 1**. Columns give the condition names (Table \@ref(tab:conds)) and rows, the model number (Table \@ref(tab:modTable)).  Empirical data from the three cohorts are represented by differently color-filled density curves from the observed discrimination performances. Simulation data are represented by an empty thick-lined density curve. The dashed line gives the median of the empirical data and the dotted line - the median of the simulated data. The discrimination performance gives the relative visitation rate of the more profitable option, or, in the incongruent condition, the option with the higher volume.  

```{r simexp1, fig.height = 7, fig.width = 10, fig.cap="(ref:labsimexp1)"}
exp1 <- exp1 %>%
  group_by(cond) %>%
  mutate(m_perf = median(performance))


ggplot() +
  geom_density(aes(performance), size = 1.2, data = sims_data %>% filter(experiment == 1)) +
  facet_grid(model ~ cond) +
  geom_density(aes(performance, fill = cohort), alpha = 0.2, data = exp1) +
  theme_bw() + scale_fill_viridis_d() +
  scale_x_continuous(name = "discrimination performance", breaks = c(0, 0.5, 1)) +
  geom_vline(aes(xintercept = m_perf), linetype = 2, data = exp1) +
  # geom_vline(xintercept = cv_fit(0.5, 0.2, cv = 0.7, lapse = 0.1), linetype = 1) +
  geom_vline(aes(xintercept = m_perf), linetype = 3, size = 1.2, data = summ_sims %>%
               filter(experiment == 1)) +
  labs(fill = "cohort") + guides(color = FALSE) + theme(strip.text.y = element_text(angle = 0))

```

(ref:labsimexp2) **Comparison of discrimination performances in all six simulation models and in the three mouse cohorts in Experiment 2**. Same notation as in Fig. \@ref(fig:simexp1).

```{r simexp2,  fig.height = 7, fig.width = 10, fig.cap="(ref:labsimexp2)"}
exp2 <- exp2 %>%
  group_by(cond) %>%
  mutate(m_perf = median(performance))

ggplot() +
  geom_density(aes(performance), size = 1.2, data = sims_data %>% filter(experiment == 2)) +
  facet_grid(model ~ cond) +
  geom_density(aes(performance, fill = cohort), alpha = 0.2, data = exp2) +
  theme_bw() + scale_fill_viridis_d() +
  scale_x_continuous(name = "discrimination performance", breaks = c(0, 0.5, 1)) +
  geom_vline(aes(xintercept = m_perf), linetype = 2, data = exp2) +
  geom_vline(aes(xintercept = m_perf), linetype = 3, size = 1.2, data = summ_sims %>%
               filter(experiment == 2)) +
  labs(fill = "cohort") + guides(color = FALSE) + theme(strip.text.y = element_text(angle = 0))

```

(ref:labsimexp3) **Comparison of discrimination performances in all six simulation models and in the three mouse cohorts in Experiment 3**. Same notation as in Fig. \@ref(fig:simexp1). 

```{r simexp3,  fig.height = 7, fig.width = 10, fig.cap="(ref:labsimexp3)"}
exp3 <- exp3 %>%
  group_by(cond) %>%
  mutate(m_perf = median(performance))

ggplot() +
  geom_density(aes(performance), size = 1.2, data = sims_data %>% filter(experiment == 3)) +
  facet_grid(model ~ cond) +
  geom_density(aes(performance, fill = cohort), alpha = 0.2, data = exp3) +
  theme_bw() + scale_fill_viridis_d() +
  scale_x_continuous(name = "discrimination performance", breaks = c(0, 0.5, 1)) +
  geom_vline(aes(xintercept = m_perf), linetype = 2, data = exp3) +
  geom_vline(aes(xintercept = m_perf), linetype = 3, size = 1.2, data = summ_sims %>%
               filter(experiment == 3)) +
  labs(fill = "cohort") + guides(color = FALSE) + theme(strip.text.y = element_text(angle = 0))

```

(ref:labsimexp4) **Comparison of discrimination performances in all six simulation models and in the three mouse cohorts in Experiment 4**. Same notation as in Fig. \@ref(fig:simexp1). 

```{r simexp4,  fig.height = 7, fig.width = 10, fig.cap="(ref:labsimexp4)"}
exp4 <- exp4 %>%
  group_by(cond) %>%
  mutate(m_perf = median(performance))

ggplot() +
  geom_density(aes(performance), size = 1.2, data = sims_data %>% filter(experiment == 1)) +
  facet_grid(model ~ cond) +
  geom_density(aes(performance, fill = cohort), alpha = 0.2, data = exp4) +
  theme_bw() + scale_fill_viridis_d() +
  scale_x_continuous(name = "discrimination performance", breaks = c(0, 0.5, 1)) +
  geom_vline(aes(xintercept = m_perf), linetype = 2, data = exp4) +
  geom_vline(aes(xintercept = m_perf), linetype = 3, size = 1.2, data = summ_sims %>%
               filter(experiment == 1)) +
  labs(fill = "cohort") + guides(color = FALSE) + theme(strip.text.y = element_text(angle = 0))

```

(ref:labnpokes) **Total number of nose pokes for each experimental condition in the three cohorts in all experiments.** Rows show different experiments (1-4). Each symbol represents the total number of nose pokes for a single mouse over one of the two experimental days of the given condition.  

```{r npokes, fig.height = 7, fig.width = 10, fig.cap="(ref:labnpokes)"}
n_pokes %>%
  ggplot(aes(cond, n, color = cohort)) +
  geom_jitter(width = 0.3, alpha = 0.7) +
  # stat_summary(fun.data = mean_cl_boot,
  #              fun.args = list(conf.int = 0.95), size = 1.5, shape = 95) +
  facet_grid(experiment ~ .) +
  ylab("total number of nose pokes") +
  xlab("condition") +
  theme_bw() +
  scale_color_viridis_d()
```

(ref:exp3models) **Slope estimates for the effect of the background dimension on the discrimination performance in the relevant dimension for different decision models**. The two choice options always differed along the relevant dimension (either probability or volume) at a fixed relative intensity. The discrimination performance for 100 virtual mice making 100 decisions each was measured at four different levels of the background dimension. Symbols and whiskers give means and 98% confidence intervals estimated from bootstraps. The smallest effect size of interest (dashed lines) was determined to be the slope that would have resulted in a difference in discrimination performance of 0.1, from the lowest to the highest level of the background dimension. Compare to Fig. \@ref(fig:exp3).  

```{r exp3models, fig.cap="(ref:exp3models)"}
bg_levs <- exp3_tab %>%
  filter(cohort == 1) %>%
  select(cond, bg_level)

slopes_lm <- sims_data %>%
  filter(experiment == 3) %>%
  mutate(dim = str_sub(cond, 1, 1),
         id = factor(id),
         model = factor(model)) %>%
  inner_join(bg_levs) %>%
  nest(-model) %>%
  mutate(fit = map(data, ~ lm(performance ~ bg_level*dim, data = .)),
         tidied = map(fit, tidy),
         confint = map(fit, confint_tidy, conf.level = 1 - 2*alpha_14)
         )

slopes_lm <- slopes_lm %>%
  unnest(tidied) %>%
  bind_cols(slopes_lm %>% 
  unnest(confint) %>%
    select(-model)) %>%
  mutate_if(is.numeric, ~round(., 4)) 
  
slopes_lm %>%
  filter(str_detect(term, "bg_level")) %>%
  mutate(estimate = ifelse(str_detect(term, ":"), lag(estimate) + estimate, estimate),
         conf.low = ifelse(str_detect(term, ":"), lag(estimate) + conf.low, conf.low),
         conf.high = ifelse(str_detect(term, ":"), lag(estimate) + conf.high, conf.high),
         term = ifelse(str_detect(term, ":"), "volume", "probability")) %>%
  ggplot() +
  geom_pointrange(aes(x = model, y = estimate, ymin = conf.low, ymax = conf.high)) +
  facet_grid(. ~ term) +
  ylab("slope estimate") +
  geom_hline(yintercept = 0, linetype = 3) + 
  geom_hline(yintercept = sl_sesoi, linetype = 2) +
  geom_hline(yintercept = -sl_sesoi, linetype = 2) +
  theme_bw()

# nose poke numbers
# miceChoices %>%
#   filter(cohort == 3) %>%
#   group_by(day, cond, IdLabel) %>%
#   summarise(n = n()) %>%
#   arrange(day, cond, IdLabel)

# feeder distributions
# miceChoices %>%
#   filter(!str_detect(cond, "r")) %>%
#   group_by(day, cond, experiment, IdLabel, cohort, loc) %>%
#   summarise(prof = mean(prof)) %>%
#   group_by(cohort, IdLabel, loc) %>%
#   summarise(prof = sum(prof, na.rm = TRUE))

# deltat_ts <- miceChoices %>%
#   # filter(rewardstatus == 1, prof == 0) %>%
#   rowwise() %>%
#   group_by(day, IdLabel, cond, experiment, prof) %>%
#   mutate(deltat_reward = DateTime - lag(DateTime),
#          cohort = factor(cohort)) %>%
#   group_by(experiment, day, cond, prof, IdLabel, cohort) %>%
#   summarise(median_deltat = median(deltat_reward, na.rm = TRUE)) 
# 
# 
# deltat_ts %>%
#   filter(cond != "explore") %>%
#   # filter(!is.na(prof)) %>%
#   ggplot(aes(cond, median_deltat, color = cohort)) +
#   # geom_jitter(width = 0.4, alpha = 0.7) +
#   stat_summary(fun.data = mean_cl_boot,
#                fun.args = list(conf.int = 0.95)) +
#   facet_grid(experiment ~ prof) +
#   # ylim(0, 1000) +
#   scale_color_viridis_d()

# total_vol %>%
#   ggplot(aes(n_vis, vol_consumed, color = IdLabel)) + geom_point() + facet_grid(cage ~ cond)
```


# Acknowledgments
We thank Miléna Brunet for data acquisition, Katja Frei for assistance with the experiments, and Alexej Schatz for software programming.   

# Authorship and contribution
V.N. conceived the study, analysed the data, and programmed the simulations. Y.W. obtained the funding. V.N., M.R., and Y.W. wrote the paper.     

# Competing interests
The authors declare that they have no competing interests.   



# References
