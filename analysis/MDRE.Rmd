---
title: "Multi-dimensional reward evaluation in mice"
header-includes:
   - \usepackage{lineno}
   - \linenumbers
   # - \usepackage{float} \floatplacement{figure}{H}
   - \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{A\arabic{figure}}}
output:   
  bookdown::pdf_document2:
    number_sections: no
    toc: no
bibliography: MDRE.bib
---
Vladislav Nachev^1,\*,&#8224;,\S^, Marion Rivalan^1,&#8224;,\S^, York Winter^1,\S^

^1^ Humboldt University, Berlin, Germany

**^\*^For correspondence:** vladislav.nachev\@charite.de  
^&#8224;^These authors contributed equally to this work

**Present Address:** ^\S^Dept. of Biology, Humboldt University, Philippstr. 13, 10099 Berlin, Germany

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Abstract

Please provide an abstract of no more than 150 words. Your abstract should explain the main contributions of your article, and should not contain any material that is not included in the main text. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras sit amet mauris in ex ultricies elementum vel rutrum dolor. Phasellus tempor convallis dui, in hendrerit mauris placerat scelerisque. Maecenas a accumsan enim, a maximus velit. Pellentesque in risus eget est faucibus convallis nec at nulla. Phasellus nec lacinia justo. Morbi fermentum, orci id varius accumsan, nibh neque porttitor ipsum, consectetur luctus risus arcu ac ex. Aenean a luctus augue. Suspendisse et auctor nisl. Suspendisse cursus ultrices quam non vulputate. Phasellus et pharetra neque, vel feugiat erat.

# Introduction

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras sit amet mauris in ex ultricies elementum vel rutrum dolor. Phasellus tempor convallis dui, in hendrerit mauris placerat scelerisque. Maecenas a accumsan enim, a maximus velit. Pellentesque in risus eget est faucibus convallis nec at nulla. Phasellus nec lacinia justo. Morbi fermentum, orci id varius accumsan, nibh neque porttitor ipsum, consectetur luctus risus arcu ac ex. Aenean a luctus augue. Suspendisse et auctor nisl. Suspendisse cursus ultrices quam non vulputate. Phasellus et pharetra neque, vel feugiat erat. Sed feugiat elit at mauris commodo consequat. Sed congue lectus id mattis hendrerit. Mauris turpis nisl, congue eget velit sed, imperdiet convallis magna. Nam accumsan urna risus, non feugiat odio vehicula eget.

# Results

```{r, include = FALSE}
library(tidyverse)
library(gridExtra)
library(grid)
library(lubridate)
library(lme4)
library(coin)
library(kableExtra)
library(Hmisc)
library(broom.mixed)

miceChoices <- read.csv2(file = "data/ChoicesOutput.csv", header = TRUE, dec = ".", sep = ";",
                            na.strings = "NA") %>%
  mutate(vol = if_else(cohort == 2, vol * 4.8, vol))

miceChoices <- miceChoices %>%
  group_by(IdLabel, day) %>%
  filter(hour(DateTime) < 9 | hour(DateTime) > 15) %>% # due to older program version the initial visits
  # before the starting could not be flagged and therefore could not be removed by CSVreader.R
  # therefore they need to be removed here
  mutate(count = 1:n(), revcount = n():1) %>%
  group_by(rel,IdLabel,day) %>%
  mutate(relcount = 1:n()) %>%
  ungroup()

summaries <- miceChoices %>%
  group_by(cond, IdLabel, experiment, cohort, rint) %>%
  # take all visits after the first 150 visits at the relevant dispensers:
  filter(relcount > 150) %>%
  # in the following lines is an alternative cut-off point, which leads to very similar results
  # take the last 100 visits at any of the dispensers:
  # filter(revcount < 101) %>%
  summarise(performance = mean(prof, na.rm = TRUE), sampling = 1 - mean(rel),
            Ntot = n(), Nrel = sum(rel),successes_perf = sum(prof, na.rm = T),
            successes_sampl =  Ntot - Nrel) %>%
  ungroup() %>%
  mutate(cohort = factor(cohort))

exp3_tab <- miceChoices %>%
  filter(experiment == 3, !str_detect(cond,"[r]")) %>%
  group_by(cohort, cond) %>%
  summarise(vol = max(volm), prob = max(prob)) %>%
  ungroup() %>%
  mutate(volm = vol/max(vol),
         probm = prob/max(prob),
         dim = str_sub(cond, 1, 1),
         bg_level = ifelse(dim == "P", volm, probm),
         cohort = as.factor(cohort)) %>%
  droplevels()

exp1 <- summaries %>% filter(!str_detect(cond, "[r]"), experiment == 1)
BVP1 <- exp1 %>%
  filter(cond == "BVP1")
exp2 <- summaries %>% filter(!str_detect(cond, "[r]"), experiment == 2) %>%
  bind_rows(BVP1)
exp3 <- summaries %>% filter(!str_detect(cond, "[r]"), experiment == 3) %>%
  inner_join(exp3_tab) %>%
  droplevels()
exp4 <- summaries %>% filter(!str_detect(cond, "[r]"), experiment == 4)

sesoi <- 0.1 # smallest effect size of interest
```

In order to test how (contradicting) information from two different dimensions is integrated and weighed, we performed a series of choice experiments (1-4, in chronological order) with mice in automated group cages [@rivalan_principles_2017]. The cages were outfitted with four computer-controlled liquid dispensers that delivered drinking water as a reward. During each of the 18h-long drinking sessions each mouse had access to all dispensers, but received rewards at only two of them. The two rewarding dispensers differed on one or both reward dimensions, probability and volume [@rivalan_principles_2017]. An overview of the differences between choice options in the different experimental conditions is given in Table \@ref(tab:conds). All experiments were conducted with three different cohorts of eight mice each. Cohort 2 was housed in a different automated group cage than cohorts 1 and 3 (See Animals, Materials, and Methods for differences between cages).

## Experiment 1  
In the baseline conditions rewards only differed on one dimension (the relevant dimension), but not on the other dimension (the background dimension). For example, in the BVP1 condition (read as baseline for volume at probability 1), both options had the same probability of 0.2, but one option had a volume of 4 $\mu$L and the other, a volume of 20 $\mu$L (Table \@ref(tab:conds)). Based on previous experiments [@rivalan_principles_2017], we expected a baseline difference between 4 $\mu$L and 20 $\mu$L volumes to result in a similar discrimination performance (relative preference for the superior option) compared to a baseline difference between probabilities 0.2 and 0.5. In the C (congruent) condition one option was superior to the other on both dimensions. Finally, in the I (incongruent) condition each of the two options was superior to the other on one of the two reward dimensions. Since the differences on both dimensions were chosen to be comparable, we expected the mean discrimination performance in the incongruent condition to be at chance level (0.5).  
In experiment 1 and in all subsequent experiments, each mouse had its own individual sequence of conditions, but each condition was followed by a reversal in the next drinking session, with a spatial inversion of the two rewarding dispensers.   

```{r conds}
conds_tab <- miceChoices %>%
  ungroup() %>%
  filter(!str_detect(cond,"[r]"), cohort != 2, between(vol, 1, 20),
         experiment < 4, prob > 0) %>%
  select(experiment, cond, vol, prob) %>%
  distinct() %>%
  arrange(experiment, cond, vol, prob)

conds_tab <- conds_tab %>%
  mutate(odd = row_number(cond) %% 2, vol2 = lead(vol), prob2 = lead(prob)) %>%
  filter(odd == 1) %>%
  select(-odd)

conds_tab %>%
  mutate(cond = if_else(cond == "BVP1",
                       paste0(cond, kableExtra::footnote_marker_alphabet(3)), as.character(cond))) %>%
  knitr::kable(booktabs = TRUE,
               caption = "Overview of the experimental conditions in all four experiments.",
               col.names = c(paste0("experiment", kableExtra::footnote_marker_alphabet(1)),
                             "condition",
                             paste0("volume", kableExtra::footnote_marker_alphabet(2)),
                             "probability",
                             paste0("volume", kableExtra::footnote_marker_alphabet(2)),
                             "probability"), escape = F) %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F) %>%
  kableExtra::add_header_above(c(" " = 2, "option A" = 2, "option B" = 2)) %>%
  kableExtra::footnote(alphabet =
                         c("conditions in experiment 1 and 4 were identical; only conditions for experiment 1 are shown here for brevity; ",
                           "the volumes (in microliters) shown are for cohorts 1 and 3. In cohort 2 the volumes were 4.8 instead of 4, 9.6 instead of 10, 14.4 instead of 15, and 20.8 instead of 20 microliters; ",
                           "condition BVP1 in experiment 1 was not repeated in experiment 2, but instead the results from experiment 1 were used in further analyses"), threeparttable = T)
```
  
(ref:labresoverview) **Overview of discrimination performance for all mice in all experiments.** Experiments 1 through 4 are shown in different panels (1-4). Each symbol is the mean discrimination performance of an individual mouse over two presentations of the same condition (original and reversal). The experimental conditions are described in detail in Table \@ref(tab:conds). The discrimination performance gives the relative visitation rate of the more profitable option, or, in the incongruent condition, the option with the higher volume. Dashed line gives the chance level of 0.5. Data are shown in different colors for three diffierent cohorts of eight mice each (total *N* = 24). Data from the same individuals are connected with lines. Cohort 2 (green symbols and lines) was tested in a different cage set-up than cohorts 1 and 2 (see Animals, Materials, and Methods for details). 

```{r resoverview, fig.cap="(ref:labresoverview)"}
ggexp12_4 <- summaries %>%
  filter(!str_detect(cond, "[r]"), experiment != 3) %>%
  ggplot() +
  geom_point(aes(cond, performance, color = cohort, group = IdLabel)) +
  geom_line(aes(cond, performance, color = cohort, group = IdLabel)) +
  facet_grid(experiment ~ .) + xlab("condition") + ylab("discrimination performance") +
  theme_bw() + scale_color_viridis_d() + guides(color = FALSE) +
  theme(strip.text.y = element_text(angle = 0)) +
  geom_hline(yintercept = 0.5, linetype = 2)
  # geom_hline(yintercept = 0.5 + sesoi, linetype = 2) +
  # geom_hline(yintercept = 0.5 - sesoi, linetype = 2) +
  # stat_summary(aes(cond, performance), fun.data = mean_cl_boot,
  #              fun.args = list(conf.int = 1 - 2*(alpha_2)),
  #              color = "darkblue", size = .5)

ggexp3 <- exp3 %>%
  ggplot() +
  geom_point(aes(cond, performance, color = cohort, group = IdLabel)) +
  geom_line(aes(cond, performance, color = cohort, group = IdLabel)) +
  facet_grid(experiment ~ .) + xlab("condition") +
  scale_y_continuous(name = "", breaks = seq(0, 1, 0.25)) +
  theme_bw() + scale_color_viridis_d() + theme(legend.position = "bottom") +
  theme(strip.text.y = element_text(angle = 0)) +
  geom_hline(yintercept = 0.5, linetype = 2)
  # geom_hline(yintercept = 0.5 + sesoi, linetype = 2) +
  # geom_hline(yintercept = 0.5 - sesoi, linetype = 2) +
  # stat_summary(aes(cond, performance), fun.data = mean_cl_boot,
  #              fun.args = list(conf.int = 1 - 2*(alpha_2)),
  #              color = "darkblue", size = .5)

get_legend <- function(a.gplot){ 
  tmp <- ggplot_gtable(ggplot_build(a.gplot)) 
  leg <- which(map(tmp$grobs, "name") == "guide-box") 
  legend <- tmp$grobs[[leg]] 
  return(legend)} 

leg <- get_legend(ggexp3)

lay <- rbind(c(1, 3),
             c(1, 2),
             c(1, NA))

grid.arrange(ggexp12_4, ggexp3 + guides(color = FALSE), leg, layout_matrix = lay,
             heights = c(1, 1.3, 1))
```

(ref:labresexp1) **Difference between discrimination performance in the baseline conditions and in the congruent and incongruent conditions in experiment 1**. Symbols show the individual differences in discrimination performance for the given conditions of each individual mouse (*N* = 24). Mice from different cohorts are shown in different colors. Large blue symbols give the means and the blue vertical lines the 90%-confidence intervals from bootstraps, corrected for multiple comparisons. When the confidence intervals lie completely within the smallest effect size of interest (sesoi) interval bounded by the dashed lines, there is statistical support for equivalence. When the confidence intervals do not cross the zero line, there is statistical support for difference. If the confidence intervals cross the zero line, but are not completely bounded by the sesoi, the results are inconclusive. The discrimination performances in the baseline conditions were calculated from the mean values from the two different baseline conditions for each reward dimension (volume and probability), i.e. BP was the mean of BPV1 and BPV2, and BV was the mean of BVP1 and BVP2 (Table \@ref(tab:conds)). The discrimination performance in the incongruent condition was calculated as the relative preference for the higher probability dispenser when contrasted with the probability baseline (I - BP) and for the higher volume dispenser when contrasted with the volume baseline (I - BV).     

```{r exp1, fig.cap="(ref:labresexp1)"}
exp1_simpl <- exp1 %>%
  ungroup() %>%
  mutate(cond = as.factor(ifelse(str_length(cond) == 4, str_sub(cond, 1, 2),
                                 as.character(cond)))) %>%
  group_by(cond, IdLabel, cohort) %>%
  summarise(performance = mean(performance))

exp1_diff <- exp1_simpl %>%
  spread(cond, performance) %>%
  mutate("BP - BV" = BP - BV, "C - BP" = C - BP, "C - BV" = C - BV,
         "I - BP" = 1 - I - BP, "I - BV" = I - BV) %>%
  select(-BV, -BP, -C, -I) %>%
  gather(cond, difference, -cohort, -IdLabel) %>%
  mutate(rel_dim = str_sub(cond, -1))

n_compare <- exp1_diff %>%
  ungroup() %>%
  filter(cond != "BP - BV") %>%
  pull(cond) %>%
  unique() %>%
  length()

set.seed(42)
mean_inc1 <- exp1 %>%
  filter(cond == "I") %>%
  pull(performance) %>%
  mean_cl_boot() %>%
  mutate_all(round, 3)


mean_ibp <- exp1_diff %>%
  filter(cond == "I - BP") %>%
  pull(difference) %>%
  mean_cl_boot() %>%
  mutate_all(round, 3)

mean_ibv <- exp1_diff %>%
  filter(cond == "I - BV") %>%
  pull(difference) %>%
  mean_cl_boot() %>%
  mutate_all(round, 3)

alpha_1 <- 0.05
alpha_2 <- 0.05 / (n_compare - 1) # alpha corrected for multiple comparisons

exp1_diff %>%
  filter(cond != "BP - BV") %>%
  ggplot(aes(cond, difference)) +
  geom_hline(yintercept = sesoi, linetype = 2) +
  geom_hline(yintercept = -sesoi, linetype = 2) +
  stat_summary(aes(cond, difference), fun.data = mean_cl_boot,
               fun.args = list(conf.int = 1 - 2*(alpha_2)),
               color = "darkblue", size = 1) +
  theme_bw() + geom_jitter(aes(color = cohort), alpha = 0.7, width = 0.1) +
  xlab("") + scale_color_viridis_d()
```

  
Compared to the baselines, mice showed an increase in discrimination performance in the congruent condition and a decrease in performance in the incongruent condition (Fig. \@ref(fig:exp1)). Contrary to our expectations, the trade-off between volume and probability did not abolish preference in the incongruent condition (Fig. \@ref(fig:resoverview)). The mean discrimination performance in the incongruent condition was higher than the chance level of 0.5 (lower 95%CI < mean < upper 95%CI, `r paste(mean_inc1$ymin, mean_inc1$y, mean_inc1$ymax, sep = " < ")`). Thus, the volume dimension exerted a stronger influence on choice, at least in absolute terms.  
 

(ref:labresexp2) **Difference between discrimination performance in the baseline conditions and in the congruent and incongruent conditions in experiment 2**. Same notation as in Fig. \@ref(fig:exp1). The discrimination performances in the baseline conditions were calculated from the mean values from the two different baseline conditions for each reward dimension (volume and probability), i.e. BP was the mean of BPV1 and BPV2, and BV was the mean of BVP1 and BVP2, where the values for condition BVP1 were taken from experiment 1 (Table \@ref(tab:conds)). The discrimination performance in the incongruent condition was calculated as the relative preference for the higher probability dispenser when contrasted with the probability baseline (I - BP) and for the higher volume dispenser when contrasted with the volume baseline (I - BV).

```{r exp2, fig.cap="(ref:labresexp2)"}
exp2_simpl <- exp2 %>%
  ungroup() %>%
  mutate(cond = as.factor(ifelse(str_length(cond) == 4, str_sub(cond, 1, 2),
                                 as.character(cond)))) %>%
  group_by(cond, IdLabel, cohort) %>%
  summarise(performance = mean(performance))

exp2_diff <- exp2_simpl %>%
  spread(cond, performance) %>%
  mutate("BP - BV" = BP - BV, "C - BP" = C - BP, "C - BV" = C - BV,
         "I - BP" = 1 - I - BP, "I - BV" = I - BV) %>%
  select(-BV, -BP, -C, -I) %>%
  gather(cond, difference, -cohort, -IdLabel)

set.seed(42)
mean_inc2 <- exp2 %>%
  filter(cond == "I") %>%
  pull(performance) %>%
  mean_cl_boot() %>%
  mutate_all(round, 3)

exp2_diff %>%
  filter(cond != "BP - BV") %>%
  ggplot(aes(cond, difference)) +
  geom_hline(yintercept = sesoi, linetype = 2) + geom_hline(yintercept = -sesoi, linetype = 2) +
  stat_summary(aes(cond, difference), fun.data = mean_cl_boot,
               fun.args = list(conf.int = 1 - 2*(alpha_2)),
               color = "darkblue", size = 1) +
  theme_bw() + geom_jitter(aes(color = cohort), alpha = 0.7, width = 0.1) +
  xlab("") + scale_color_viridis_d()

```

## Experiment 2
In previous experiments [@rivalan_principles_2017], we had shown that the relative stimulus intensity, i.e. the absolute difference between two options divided by their mean, was a good predictor of discrimination performance for both volume and probability differences. Another finding from these experiments was that, at least initially, mice responded less strongly to  differences in volume than to differences in probability, despite equivalence in return rates [@rivalan_principles_2017]. We tried to correct for this in experiment 1 by selecting options with a higher relative intensity for volume (4 $\mu$L vs. 20 $\mu$L, r.int. = 1.33) than for probability (0.2 vs. 0.5, r.int. = 0.857). In order to test whether we had overcorrected for decreased sensitivity to volume in experiment 1, we performed a slightly modified version, experiment 2, which was the same, but with probability of 1 instead of 0.5 in every choice option (Table \@ref(tab:conds)). Thus, with the two choice options having the same relative intensities (r.int. = 1.33) and being equivalent in return rates, we expected the discrimination performance in the incongruent condition to be at chance level if both dimensions were equally weighed and equally perceived. On the other hand, if mice were less sensitive for volume than for probability differences as in our previous experiments, then the discrimination performance in the incongruent condition should be skewed towards probability (< 0.5).

In contrast to experiment 1, in experiment 2 mice showed an increase in discrimination performance in the congruent condition only when compared to the volume baseline, but not when compared to the probability baseline (Fig. \@ref(fig:exp2)). As in experiment 1, the discrimination performance in the incongruent condition was lower than in either of the two baselines (Fig. \@ref(fig:exp2)). Although the discrimination performance in the incongruent condition was again different from 0.5 (`r paste(mean_inc2$ymin, mean_inc2$y, mean_inc2$ymax, sep = " < ")`), it was lower than chance, thus skewed towards probability (Fig. \@ref(fig:resoverview)).


## Experiment 3
In the previous experiments we used two different baseline conditions for each dimension (BPV1, BPV2, BVP1, and BVP2, Table \@ref(tab:conds)), in order to exhaust all combinations of reward stimuli and balance the experimental design. But could it be that the level of the background dimension despite being the same across choice options nevertheless affects the discrimination performance on the relevant dimension? Researches have proposed that in multidimensional choice the decision process can be considerably simplified if differences that are (nearly) equal are are not evaluated but ignored [@tversky_intransitivity_1969; @shafir_intransitivity_1994; @shafir_comparative_2014]. Thus we can predict that regardless of the level of the background dimension, the discrimination performance on the relevant dimension should remain constant. Alternatively, all information from all reward dimensions is used for the calculation of a single value (utility) [@tversky_intransitivity_1969; @shafir_intransitivity_1994; @shafir_comparative_2014]. Since the utility curve is generally assumed to be decelerating [@kahneman_prospect_1979, but see also @kacelnik_risky_1998], we may expect that as the background dimension increases the subjective difference between the options will decrease and the discrimination performance will also decrease as a result. In order to test whether the two reward dimensions volume and probability interact with each other even when one of them is irrelevant (being the same across choice options), we performed experiment 3.  

The conditions in experiment 3 were chosen to be like the background conditions in the previous experiments, by having one background and one relevant dimension (Table \@ref(tab:conds)). There were four different levels for each background dimension (volume and probability). Each mouse had its own pseudo-random sequence of the eight possible conditions. As in all other experiments, each condition was followed by a reversal. 

(ref:labresexp3) **Slope estimates for the effect of the background dimension on the discrimination performance in the relevant dimension**. The two choice options always differed along the relevant dimension (either probability or volume, given on the abscissa) at a fixed relative intensity (rel.int. = 0.86 for probability and 0.67 for volume). The discrimination performance for each mouse was measured at four different levels of the background dimension, which was set at the same values on both choice options during a single drinking session, but differed from condition to condition (Table \@ref(tab:conds)). Each symbol is the average for an individual mouse over the two presentations of the same condition (original and reversal). The smallest effect size of interest was determined to be the slope that would have resulted in a difference in discrimination performance of 0.1, from the lowest to the highest level of the background dimension. The remaining notation is the same as in Fig. \@ref(fig:exp1).   

```{r exp3, fig.cap="(ref:labresexp3)"}

slopes <- exp3 %>%
  split(.$dim) %>%
  map(~ lmer(performance ~ bg_level + (bg_level|IdLabel), data = .)) %>%
  map(coef) %>%
  map("IdLabel") %>%
  map_df(~rownames_to_column(., var = "IdLabel"))

dims <- data.frame(dimension = rep(c("prob", "vol"), each = 24))

cohorts <- exp3 %>%
  ungroup() %>%
  select(IdLabel, cohort) %>%
  distinct()

slopes <- slopes %>%
  bind_cols(dims) %>%
  rename(slope = bg_level) %>%
  inner_join(cohorts)

set.seed(42)
CIs <- slopes %>%
  nest(-dimension) %>%
  mutate(test = map(data, ~ mean_cl_boot(.x$slope, conf.int = 1 - 2*(alpha_1)))) %>%
  unnest(test) %>%
  select(-data)

prob_slope <- CIs %>%
  filter(dimension == "prob") %>%
  pull(ymin) %>%
  round(3)

mean_PV1 <- exp3 %>%
  filter(cond == "PV1") %>%
  summarise(performance = mean(performance)) %>%
  pull(performance)

pred_PV4 <- slopes %>%
  filter(dimension == "prob") %>%
  summarise(intercept = mean(`(Intercept)`)) + prob_slope

sl_sesoi <- data.frame(val = c(4/20, 20/20),
                       performance = c(0.5, 0.6))

sl_sesoi <- coef(lm(performance ~ val, data = sl_sesoi))[2]

ggplot() +
  geom_pointrange(data = CIs, aes(x = dimension, y = y, ymin = ymin, ymax = ymax),
                  color = "darkblue", size = 1) +
  geom_hline(yintercept = sl_sesoi, linetype = 2) +
  geom_hline(yintercept = -sl_sesoi, linetype = 2) +
  theme_bw() +
  geom_jitter(data = slopes,
              aes(dimension, slope, color = cohort), alpha = 0.7, width = 0.1) +
  scale_color_viridis_d()

```
  
In experiment 3 the slope estimate over the different conditions was equivalent to zero for the volume dimension, but not for the probability dimension (Fig. \@ref(fig:resoverview), Fig. \@ref(fig:exp3)). In the probability dimension the lower confidence interval for the slope estimate was `r round(prob_slope, 3)`, just outside of the sesoi (`r -sl_sesoi`), indicating a significantly negative slope. To help illustrate the effect size, let us first consider that the typical mouse has a relative preference of `r round(mean_PV1, 3)` for probability 0.5 over probability 0.2 (both giving 4 $\mu$L rewards). With the estimated slope the predicted performance for the same probabilities but both at 20 $\mu$L is `r round(pred_PV4, 3)`.
<!-- The relative intensities for the two dimensions were the same in cohorts 1,3 (rint = 0.86), but different in cohort 2 (rintp = 0.86, rintv = 0.67). -->

```{r}

sims_data <- map(list("data/summ_Exp1.csv", "data/summ_Exp2.csv",
                       "data/summ_Exp3.csv", "data/summ_Exp1.csv"),
                     ~ read.csv2(file = ., header = TRUE, dec = ".", sep = ";",
                                na.strings = "NA")) %>%
  map2_df(1:4, ~update_list(.x, experiment = .y)) %>%
  rename(cond = cond_names)
  
summ_sims <- sims_data %>%
  group_by(experiment, Choice_type, cond) %>%
  summarise(medperf = median(performance))

emp_perf <- summaries %>%
  filter(!str_detect(cond, "[r]")) %>%
  select(IdLabel, cohort, experiment, cond, performance)

devs <- summ_sims %>%
  full_join(emp_perf) %>%
  mutate(deviance = (medperf - performance)^2)

rmses <- devs %>%
  group_by(experiment, Choice_type) %>%
  summarise(RMSE = sqrt(mean(deviance, na.rm = TRUE))) %>%
  arrange(experiment, RMSE)

# rmses_inds <- devs %>%
#   group_by(IdLabel, cohort, Choice_type) %>%
#   summarise(RMSE = sqrt(mean(deviance))) %>%
#   filter(!is.na(IdLabel)) %>%
#   mutate(rank = rank(RMSE)) %>%
#   arrange(IdLabel, rank)
# 
# devs %>%
#   group_by(cohort, Choice_type) %>%
#   summarise(RMSE = sqrt(mean(deviance))) %>%
#   filter(!is.na(cohort)) %>%
#   mutate(rank = rank(RMSE)) %>%
#   arrange(cohort, rank)
# rmses_inds  %>%
#   group_by(rank, Choice_type) %>%
#   count() %>%
#   arrange(rank, desc(n))

ranks <- rmses %>%
  mutate(rank = rank(RMSE)) %>%
  ungroup() %>%
  select(-RMSE) %>%
  spread(experiment, Choice_type) 
```
## Experiment 4


## Model comparison

Compared to the empirical data, models 2 and 7 performed best (Tables \@ref(tab:modTable), \@ref(tab:simRankTable), see also Appendix 1 Figures \@ref(fig:simexp1), \@ref(fig:simexp2), \@ref(fig:simexp3), and \@ref(fig:simexp4)).

```{r modTable}
data.frame(model = 1:7,
           description = c("hurdle", "random dimension",
                           "winner takes it all", "probability first",
                           "volume first", "only volume",
                           "logproduct rule")) %>%
  knitr::kable(booktabs = TRUE, caption = "Decision-making models") %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)
```

```{r simRankTable}
ranks %>%
  knitr::kable(booktabs = TRUE,
               caption = "Best performing models ranked by root-mean-square-deviations (RMSD).") %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F) %>%
  kableExtra::add_header_above(c(" " = 1, "experiment" = 4)) %>%
  kableExtra::footnote(general = "The deviations for the RMSD were calculated from the median of the model to the observed discrimination performances of the experimental mice.", threeparttable = T)
```


# Discussion

The effect of the background level of probability on volume in Experiment 3 was only present if the probability of 0.2 was included, otherwise discrimination performance was unaffected by probability.

## Difference between cohorts

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras sit amet mauris in ex ultricies elementum vel rutrum dolor. Phasellus tempor convallis dui, in hendrerit mauris placerat scelerisque. Maecenas a accumsan enim, a maximus velit. Pellentesque in risus eget est faucibus convallis nec at nulla. Phasellus nec lacinia justo. Morbi fermentum, orci id varius accumsan, nibh neque porttitor ipsum, consectetur luctus risus arcu ac ex. Aenean a luctus augue. Suspendisse et auctor nisl. Suspendisse cursus ultrices quam non vulputate. Phasellus et pharetra neque, vel feugiat erat. Sed feugiat elit at mauris commodo consequat. Sed congue lectus id mattis hendrerit. Mauris turpis nisl, congue eget velit sed, imperdiet convallis magna. Nam accumsan urna risus, non feugiat odio vehicula eget.

# Animals, Methods, and Materials

The different experimental conditions for all animals and cohorts are listed in Table \@ref(tab:conds).



Guidelines can be included for standard research article sections, such as this one. 

Any "personal communications" relating to unpublished data should be incorporated within the main text, in the following format: (Author Initial(s) and Surname, personal communication, Month and Year). Authors should have permission from anyone named in this way and should be aware that a supporting letter will sometimes be requested.
Within the Materials and Methods and/or figure legends, we encourage authors to provide complete information about their experiments, analyses, or data collection to ensure that readers can easily understand what was measured and analysed, and can accurately perform the relevant protocols.
In cases where a new method within the submission would benefit from step-by-step protocols in addition to the methods described in the article, we would encourage authors to also consider submitting a detailed protocol to Bio-protocol.
On first mention, please provide details of any manufacturers in the following format: company name, city, country (or state, if based in the United States).

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras sit amet mauris in ex ultricies elementum vel rutrum dolor. Phasellus tempor convallis dui, in hendrerit mauris placerat scelerisque. Maecenas a accumsan enim, a maximus velit. Pellentesque in risus eget est faucibus convallis nec at nulla. Phasellus nec lacinia justo. Morbi fermentum, orci id varius accumsan, nibh neque porttitor ipsum, consectetur luctus risus arcu ac ex. Aenean a luctus augue. Suspendisse et auctor nisl. Suspendisse cursus ultrices quam non vulputate. Phasellus et pharetra neque, vel feugiat erat. Sed feugiat elit at mauris commodo consequat. Sed congue lectus id mattis hendrerit. Mauris turpis nisl, congue eget velit sed, imperdiet convallis magna. Nam accumsan urna risus, non feugiat odio vehicula eget.

\newpage
# Appendix {-}

\beginsupplement

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras sit amet mauris in ex ultricies elementum vel rutrum dolor. Phasellus tempor convallis dui, in hendrerit mauris placerat scelerisque. Maecenas a accumsan enim, a maximus velit. Pellentesque in risus eget est faucibus convallis nec at nulla. Phasellus nec lacinia justo. Morbi fermentum, orci id varius accumsan, nibh neque porttitor ipsum, consectetur luctus risus arcu ac ex. Aenean a luctus augue. Suspendisse et auctor nisl. Suspendisse cursus ultrices quam non vulputate. Phasellus et pharetra neque, vel feugiat erat. Sed feugiat elit at mauris commodo consequat. Sed congue lectus id mattis hendrerit. Mauris turpis nisl, congue eget velit sed, imperdiet convallis magna. Nam accumsan urna risus, non feugiat odio vehicula eget.


(ref:labappfig1) **Difference in discrimination performance between identical conditions in experiment 1 and experiment 4**. Same notation as in in Fig. \@ref(fig:exp1). The sequence of conditions was pseudo-random in each experiment and different for each individual. Negative differences indicate an increase in discrimination performance with time. Mice were seven weeks old at the beginning of experiment 1 and 13-14 weeks old at the beginning of experiment 4 (a difference of about 7 weeks). The discrimination performance in the incongruent condition was calculated as the relative preference for the higher volume dispenser.    

```{r appfig1, fig.cap="(ref:labappfig1)"}
alpha_14 <- 0.05/5

exp14 <- summaries %>%
  filter(!str_detect(cond, "[r]"), experiment == 1 | experiment == 4) %>%
  mutate(rep = ifelse(experiment == 1, 1, 2)) %>%
  ungroup() %>%
  mutate(rep = factor(rep))

exp14_simpl <- exp14 %>%
  ungroup() %>%
  # mutate(cond = as.factor(ifelse(str_length(cond) == 4, str_sub(cond, 1, 2),
  #                                as.character(cond)))) %>%
  group_by(cond, IdLabel, cohort, rep) %>%
  summarise(performance = mean(performance))

# exp14_simpl %>%
#   ggplot(aes(cond, performance)) + geom_point(aes(color = cohort, shape = rep))

exp14_diff <- exp14_simpl %>%
  spread(rep, performance) %>%
  mutate(difference = `1` - `2`)

set.seed(42)
exp14_diff %>%
  ggplot() +
  geom_hline(yintercept = 0.1, linetype = 2) + geom_hline(yintercept = -0.1, linetype = 2) +
  stat_summary(aes(cond, difference, color = NULL), fun.data = mean_cl_boot,
               fun.args = list(conf.int = 1 - 2 * alpha_14),
               color = "darkblue", alpha = 0.7, size = 1) +
  theme_bw() + geom_jitter(aes(cond, difference, color = cohort), alpha = 0.7, width = 0.1) +
  scale_color_viridis_d()

```


(ref:labappexp4) **Difference between discrimination performance in the baseline conditions and in the congruent and incongruent conditions in experiment 4**. Same notation as in Fig. \@ref(fig:exp1). The discrimination performance in the incongruent condition was calculated as the relative preference for the higher probability dispenser when comparing to the probability baseline and for the higher volume dispenser when comparing to the volume baseline.  

```{r appexp4, fig.cap="(ref:labappexp4)"}
exp4_simpl <- exp14_simpl %>%
  filter(rep == 2) %>%
  ungroup() %>%
  mutate(cond = as.factor(ifelse(str_length(cond) == 4, str_sub(cond, 1, 2),
                                 as.character(cond)))) %>%
  group_by(cond, IdLabel, cohort) %>%
  summarise(performance = mean(performance))

exp4_diff <- exp4_simpl %>%
  spread(cond, performance) %>%
  mutate("BP - BV" = BP - BV, "C - BP" = C - BP, "C - BV" = C - BV,
         "I - BP" = 1 - I - BP, "I - BV" = I - BV) %>%
  select(-BV, -BP, -C, -I) %>%
  gather(cond, difference, -cohort, -IdLabel)

set.seed(42)
exp4_diff %>%
  filter(cond != "BP - BV") %>%
  ggplot(aes(cond, difference)) +
  geom_hline(yintercept = sesoi, linetype = 2) + geom_hline(yintercept = -sesoi, linetype = 2) +
  stat_summary(aes(cond, difference), fun.data = mean_cl_boot,
               fun.args = list(conf.int = 1 - 2*(alpha_2)),
               color = "darkblue", size = 1) +
  theme_bw() + geom_jitter(aes(color = cohort), alpha = 0.7, width = 0.1) +
  xlab("") + scale_color_viridis_d()
```




(ref:labsimexp1) **Comparison of discrimination performances in all seven simulation models and in the three mouse cohorts in Experiment 1**. Columns give the condition names (Table \@ref(tab:conds)) and rows, the model number (Table \@ref(tab:modTable)).  Empirical data from the three cohorts are represented by differently color-filled density curves from the observed discrimination performances. Simulation data are represented by an empty thick-lined density curve. The dashed line gives the median of the empirical data and the dotted line - the median of the simulated data. The discrimination performance gives the relative visitation rate of the more profitable option, or, in the incongruent condition, the option with the higher volume.  

```{r simexp1, fig.height = 7, fig.width = 10, fig.cap="(ref:labsimexp1)"}
exp1 <- exp1 %>%
  group_by(cond) %>%
  mutate(medperf = median(performance))

ggplot() +
  geom_density(aes(performance), size = 1.2, data = sims_data %>% filter(experiment == 1)) +
  facet_grid(Choice_type ~ cond) +
  geom_density(aes(performance, fill = cohort), alpha = 0.2, data = exp1) +
  theme_bw() + scale_fill_viridis_d() +
  scale_x_continuous(name = "discrimination performance", breaks = c(0, 0.5, 1)) +
  geom_vline(aes(xintercept = medperf), linetype = 2, data = exp1) +
  geom_vline(aes(xintercept = medperf), linetype = 3, size = 1.2, data = summ_sims %>%
               filter(experiment == 1)) +
  labs(fill = "cohort") + guides(color = FALSE) + theme(strip.text.y = element_text(angle = 0))

```

(ref:labsimexp2) **Comparison of discrimination performances in all seven simulation models and in the three mouse cohorts in Experiment 2**. Same notation as in Fig. \@ref(fig:simexp1).

```{r simexp2,  fig.height = 7, fig.width = 10, fig.cap="(ref:labsimexp2)"}
exp2 <- exp2 %>%
  group_by(cond) %>%
  mutate(medperf = median(performance))

ggplot() +
  geom_density(aes(performance), size = 1.2, data = sims_data %>% filter(experiment == 2)) +
  facet_grid(Choice_type ~ cond) +
  geom_density(aes(performance, fill = cohort), alpha = 0.2, data = exp2) +
  theme_bw() + scale_fill_viridis_d() +
  scale_x_continuous(name = "discrimination performance", breaks = c(0, 0.5, 1)) +
  geom_vline(aes(xintercept = medperf), linetype = 2, data = exp2) +
  geom_vline(aes(xintercept = medperf), linetype = 3, size = 1.2, data = summ_sims %>%
               filter(experiment == 2)) +
  labs(fill = "cohort") + guides(color = FALSE) + theme(strip.text.y = element_text(angle = 0))

```

(ref:labsimexp3) **Comparison of discrimination performances in all seven simulation models and in the three mouse cohorts in Experiment 3**. Same notation as in Fig. \@ref(fig:simexp1). 

```{r simexp3,  fig.height = 7, fig.width = 10, fig.cap="(ref:labsimexp3)"}
exp3 <- exp3 %>%
  group_by(cond) %>%
  mutate(medperf = median(performance))

ggplot() +
  geom_density(aes(performance), size = 1.2, data = sims_data %>% filter(experiment == 3)) +
  facet_grid(Choice_type ~ cond) +
  geom_density(aes(performance, fill = cohort), alpha = 0.2, data = exp3) +
  theme_bw() + scale_fill_viridis_d() +
  scale_x_continuous(name = "discrimination performance", breaks = c(0, 0.5, 1)) +
  geom_vline(aes(xintercept = medperf), linetype = 2, data = exp3) +
  geom_vline(aes(xintercept = medperf), linetype = 3, size = 1.2, data = summ_sims %>%
               filter(experiment == 3)) +
  labs(fill = "cohort") + guides(color = FALSE) + theme(strip.text.y = element_text(angle = 0))

```

(ref:labsimexp4) **Comparison of discrimination performances in all seven simulation models and in the three mouse cohorts in Experiment 4**. Same notation as in Fig. \@ref(fig:simexp1). 

```{r simexp4,  fig.height = 7, fig.width = 10, fig.cap="(ref:labsimexp4)"}
exp4 <- exp4 %>%
  group_by(cond) %>%
  mutate(medperf = median(performance))

ggplot() +
  geom_density(aes(performance), size = 1.2, data = sims_data %>% filter(experiment == 1)) +
  facet_grid(Choice_type ~ cond) +
  geom_density(aes(performance, fill = cohort), alpha = 0.2, data = exp4) +
  theme_bw() + scale_fill_viridis_d() +
  scale_x_continuous(name = "discrimination performance", breaks = c(0, 0.5, 1)) +
  geom_vline(aes(xintercept = medperf), linetype = 2, data = exp4) +
  geom_vline(aes(xintercept = medperf), linetype = 3, size = 1.2, data = summ_sims %>%
               filter(experiment == 1)) +
  labs(fill = "cohort") + guides(color = FALSE) + theme(strip.text.y = element_text(angle = 0))

```

# Acknowledgments
Individuals who have contributed materially to the work, but do not satisfy the authorship criteria should be listed in the acknowledgements section. Authors should seek permission to include any individuals mentioned in the acknowledgements.  

# Competing interests
At this stage we request that the corresponding author provides a statement of financial and non-financial competing interests on behalf of all authors. Examples include paid employment or consultancy, stock ownership, patent applications, personal relationships with relevant individuals, and membership of an advisory board.



# References
